\section{Background} \label{background}
To continuously train the deployed model, we rely on computing partial updates based on the current parameters of the model and a combination of the incoming and existing data.
To compute the partial updates, we utilize Stochastic Gradient Descent (SGD) \cite{zhang2004solving}.
SGD has several parameters and in order to work effectively, they have to be tuned.
In this section, we describe the details of SGD and its parameters and discuss the effect of the parameters on training machine learning models.

\subsection{Stochastic Gradient Descent} \label{sgd}
\textit{Stochastic Gradient Descent (SGD)} is an optimization strategy utilized by many machine learning algorithms for training a model.
SGD is an iterative optimization technique where in every iteration, one data point or a sample of the data points is utilized to update the model.
SGD is suitable for large datasets as it does not require scanning the entire data in every iteration \cite{bottou2010large}.
Many different machine learning tasks such as classification \cite{zhang2004solving, macmahan2013}, clustering \cite{bottou1995convergence}, and matrix factorization \cite{koren2009matrix,  funk2006netflix}, are utilizing SGD in training models.
SGD is also the most common optimization strategy for training neural networks on large datasets \cite{dean2012large}.
Prominent applications of SGD in neural networks are the work of Google Deepmind team that managed to train neural networks that defeat humans in the game of Go \cite{silver2016mastering} and mastering Atari games \cite{mnih2013playing}.

To explain the details of SGD, we describe how it is utilized to train a logistic regression model.
In logistic regression, the goal is to find the weight vector ($w$) that maximizes the conditional likelihood of labels ($y$) based on the given data ($x$) in the training dataset:

\begin{center}
$$w^* = \argmax_w \sum_{i=1}^{N} ln(P(y^i | x^i, w))$$
\end{center}

where $N$ is the size of the training dataset.
To utilize SGD for finding the optimal $w$, we start from initial random weights.
Then in every iteration, we update the weights based on the gradient of the loss function:

\begin{center}
$${w}^{t+1} = {w}^t + \eta \sum_{i \in S} x^i (y^i - \hat{P}(Y^i = 1 | x^i w))$$
\end{center}

where $\eta$ is the learning rate parameter and $S$ is the random sample in the current iteration.
The algorithm continues until convergence (when the weight vector does not change after an iteration).

\textbf{Learning Rate.}
An important parameter of stochastic gradient descent is the learning rate.
The learning rate controls the degree of change in the weights, during every iteration.
The most trivial approach for tuning the learning rate is to initialize it to a small value and after every iteration decrease the value by a small factor.
However, in complex and high-dimensional problems, the simple tuning approach is ineffective \cite{schaul2013no}. 
Adaptive learning rate methods such as Momentum \cite{qian1999momentum}, Adam \cite{kingma2014adam}, RMSPROP \cite{tieleman2012lecture}, and AdaDelta \cite{zeiler2012adaptive} have been proposed.
These methods automatically dynamically adjust the learning rate in every iteration to speed up the convergence rate.
Moreover, some of the learning rate adaptation methods perform per coordinate modification, i.e., every parameter of the model weight vector is adjusted separately from the others \cite{schaul2013no, tieleman2012lecture, zeiler2012adaptive}. 
In many high-dimensional problems, the parameters of the weight vector do not have the same level of importance, therefore each parameter must be treated differently during the training process.

\textbf{Sample Size.}
Another parameter of stochastic gradient descent is the sample size.
Given proper learning rate tuning mechanism, SGD is guaranteed to converge to a solution regardless of the sample size.
However, the sample size can greatly affect the time that is required to converge.
Two extremes of the sample size are 1 (every iteration considers 1 data item) and $N$ (similar to batch gradient descent, every iteration scans the entire dataset).
Setting the sample size to 1 increases the model update frequency, however, it results in noisy updates.
Therefore, more iterations are required for the model to converge.
Using the entire data in every iteration leads to more stable updates.
As a result, the model training process requires fewer iterations to converge.
However, because of the size of the data, individual iterations require more time to complete.
\hl{The most common approach is to set the sample size to a value small enough so that each sample can be processed quickly but large enough so the updates are not noisy (called a mini-batch gradient descent).}

\textbf{Distributed SGD.}
To efficiently train machine learning models on large datasets, one has to employ scalable training algorithms.
SGD inherently works well with large datasets because it does not need to scan every data point during every iteration.
However, SGD has to perform many iterations to converge.
To decrease the execution time, one can distribute the large dataset among multiple nodes.
During the training, each node computes a partial gradient on a subset of the data in parallel.
After this step, all the partial gradients are combined to compute the final gradient.
Distributed SGD significantly reduces the time for executing individual iterations, which results in a reduction in the overall training time.
