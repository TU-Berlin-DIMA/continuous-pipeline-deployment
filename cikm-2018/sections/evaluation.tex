\section{Evaluation} \label{evaluation} 
To evaluate the performance of our deployment platform, we perform several experiments.
The goal of the experiments is to answer the following questions:\\
1. How does our continuous deployment approach perform in comparison to online and periodical deployment approaches with regards to model quality and training time? \\
2. What are the effects of the learning rate adaptation method, the regularization parameter, and the sampling method on the continuous deployment? \\
3. What are the effects of live statistics analysis and materialization of preprocessed features on the training time?

To that end, first, we design several pipelines each processing one real-world dataset.
Then, we deploy the pipelines using different deployment approaches.

\subsection{Setup}\label{subsec:setup}
\textbf{Pipelines.}
We design three pipelines for all the experiments.

\textit{URL pipeline. } The URL pipeline consists of 5 components: input parser, missing value imputer, standard scaler, feature hasher, and an SVM model.
We use the URL pipeline to process the URL dataset for classifying URLs, gathered over a 121 days period, into malicious and legitimate groups \cite{ma2009identifying}.\\
To evaluate the SVM model, we compute the misclassification rate on the unseen data.

\textit{Criteo Pipeline.} 
The Criteo pipeline consists of 5 components: input parser, missing value imputer, standard scaler, feature hasher, and a Logistic Regression model.
The Criteo pipeline processes the Terabyte Criteo click log dataset and predicts the clickthrough rate (CTR) for each data point \cite{criteo-log}.\\
We use the Mean Squared Error (MSE) to capture the error rate of the Logistic Regression model.

\textit{Taxi Pipeline.}
The Taxi pipeline consists of 5 components: input parser, feature extractor, anomaly detector, standard scaler, and a Linear Regression model.
The Taxi pipeline processes the Newyork taxi trip dataset and predicts the trip duration of every taxi ride \cite{newyork-taxi}. 
The Taxi pipeline is more complex than the Criteo and the URL pipelines. 
We designed the pipeline based on the solutions of the top scorers of the New York City Taxi Trip Duration Kaggle competition\footnote{https://www.kaggle.com/c/nyc-taxi-trip-duration/}. 
The input parser computes the actual trip duration by first extracting the pickup and drop off time fields from the input records and calculating the difference (in seconds) between the two values.
The feature extractor computes the haversine distance\footnote{https://en.wikipedia.org/wiki/Haversine\_formula}, the bearing\footnote{https://en.wikipedia.org/wiki/Bearing\_(navigation)}, the hour of the day, and the day of the week from the input records. 
Finally, the anomaly detector filters the trips that are longer than 22 hours, smaller than 10 seconds, or the trips that have a total distance of zero (the car never moved).
To evaluate the model, we use the Root Mean Squared Logarithmic Error (RMSLE) measure.
RMSLE is also the chosen error metric for the NYC taxi Kaggle competition.

\textbf{Deployment Environment. }
\todo[inline]{include the url result from the cluster}
We deploy the URL pipeline on a single laptop running a macOS with 2,2 GHz intel Core i7 and 16 GB of RAM and the Criteo and the Taxi pipelines on a cluster of 21 machines (intel Xeon 2.4 GHz 16 cores, 28 GB of dedicated RAM per node).
Both environments are using Apache Spark 2.2.0 as execution engine.

\textbf{Deployment Process. }
Table \ref{dataset-description} describes the details of the datasets, the amount of data for the initial training, and the amount of data for prediction requests and further training after deployment. 
For both the URL and the Criteo pipeline, we first train a model on the first day of the data (day 0).
For the Taxi pipeline, we train a model using the data from January 2015.
We then deploy the models (and the pipelines).
We use the remaining data for sending prediction queries and further training of the deployed models.
Using the prediction queries, we compute the cumulative prequential error rate over time to evaluate the performance of each model during the deployment \cite{dawid1984present}.
The URL and the Criteo datasets do not have timestamps, therefore, we divide every day of the data into micro-batches of 1 minute.
The total number of micro-batches for the URL and the Criteo datasets are 12,000 and XX, respectively.
We send each micro-batch to the deployment platform where first they are used for prequential evaluation and then for training the model.
The NYC taxi dataset includes timestamps. 
In our experiments, each micro-batch of the NYC taxi dataset contains one hour of the data. 
The micro-batches are sent in order of the timestamps (from 2015-Feb-01  00:00 to 2016-Jun-31 24:00, a 2 year period) to the deployment platform.

\todo[inline]{numbers for taxi and criteo}
\begin{table}[h!]
\centering
\begin{tabular}{lrrll}
\hline
\textbf{Dataset}  & \textbf{size} &\textbf{\# instances} & \textbf{Initial} & \textbf{Deployment} \\
\hline
URL        &  2.21 GB & 2.3 M           & Day 0        & Day 1-120          \\
Taxi        & 30.44 GB & XX B              & Jan15             & Feb15 to Jul16    \\
Criteo       &  & XX B            & Day 0           & Day 1-10       \\
\hline
\end{tabular}
\caption{Description of Datasets. The \textbf{Initial} column indicates the amount of data used during the initial model training and the \textbf{Deployment} column indicates the amount of data used for both prequential evaluation and further model training}  
\label{dataset-description}
\end{table}


\subsection{EX1: Deployment Modes}
In this experiment, we investigate the effect of our continuous deployment approach on model quality and the total training time.
We use 4 different deployment approaches.
\begin{itemize}
\item Baseline: deploy the pipeline with no further training
\item Online: deploy the pipeline and only use online (gradient descent) method for updating the model
\item Periodical: deploy the pipeline and periodically train new models 
\item Continuous: deploy the pipeline and continuously update the model using our framework
\end{itemize}

In order to improve the performance of the periodical deployment, we utilize the warm starting technique, used in the TFX framework \cite{baylor2017tfx}.
In warmstarting, each periodical training uses the existing parameters such as the pipeline statistics (e.g, standard scaler), model weights, and learning rate adaptation parameters (e.g, the average of past gradients used in Adadelta, Adam, and Rmsprop) when training new models.

\textbf{Model Quality. }
Figure \ref{deployment-quality-figure} shows the cumulative error rate over time for the different deployment approaches.
For the URL pipeline, the periodical deployment approach initiates a full retraining every 10 days.
Both the continuous and the periodical deployment approaches result in lower misclassification rates than the online deployment approach.
During the first 60 days, the continuous deployment results in a slightly lower misclassification rate than the periodical approach.
After the 6th periodical training (day 60), the misclassification rate of the periodical deployment decreases slightly more rapidly than that of the continuous deployment. 
At the end of the deployment process (day 120), the misclassification rate of the periodical deployment approach is 0.03\% smaller than the URL model deployed using the continuous deployment approach.
\begin{figure}[h!]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/deployment-quality-experiment}}
\caption{Model Quality for different deployment approaches}
\label{deployment-quality-figure}
\end{figure}

\textbf{Total Training Time. }
Figure \ref{deployment-time-figure} shows the end-to-end  time for each deployment approach.
The end-to-end time consists of the data preprocessing and feature engineering time, the model training time, and the prediction time. 
We include the baseline deployment approach, as it only includes the prediction time.
For the URL pipeline, the end-to-end time for the online deployment approach is 29 minutes as it only scans each data point once (around 2.4 million scans).  
The proactive training of the continuous deployment approach makes over 45 million scans.
However, the end-to-end time of the continuous deployment approach is only two times larger than the online deployment approach. 
Because of the live data analysis and the feature materialization optimization, a large part of the data processing time is avoided.
The periodical deployment approach time is around 800 minutes, an order of magnitude large than the continuous deployment. 
The overhead of data processing and the model retraining contributes to the large end-to-end deployment time.
\todo[inline]{Tilmann: You should keep the same order in all experiments and yours should be last and best. }
\begin{figure}[h!]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/deployment-time-experiment}}
\caption{End-to-end deployment time for different deployment approaches}
\label{deployment-time-figure}
\end{figure}

\subsection{EX2: System Tuning}
\todo[inline]{Tilmann: What exactly do you wan to show with this? Seems to me that you could summarize the result rather than showing the full experiment since you are just trying out different existing algorithms...}
\begin{table*}[t]
\centering
\begin{adjustbox}{max width=\textwidth}
\input{../images/experiment-results/tikz/parameter-selection-table}
\end{adjustbox}
\caption{Hyperparameter tuning during initial training (bold numbers show the best result for a dataset)}
\label{hyper-param-table}
\end{table*}

In this experiment, we investigate the effect of different parameters on the quality of the models after deployment.
We leverage the well-established hyperparameter tuning technique in the machine learning literature to tune the parameters of proactive training.
As described in Section \ref{proactive-training}, proactive training is an extension of the stochastic gradient descent to the deployment phase.
Therefore, we expect the set of hyperparameters with best performance during the initial training also performs the best during the deployment phase.
Next, we investigate the effect of different sampling techniques on the quality of the deployed model.

\textbf{Proactive Training Parameters. }
Stochastic gradient descent is heavily dependent on the choice of learning rate and the regularization parameter.
To find the best set of hyperparameters for the initial training, we perform a grid search.
Instead of using a constant learning rate, we use advanced learning rate adaptation techniques (Adam, Adadelta, and RMSPROP).
Therefore, during our hyperparameter search process, instead of trying different learning rate values, we search for the best adaptation technique for every dataset.
For each dataset, we divide the initial data (from Table \ref{dataset-description}) into a training and evaluation set.
For each configuration, we first train a model using the training set and then evaluate the model using the evaluation set.
Table \ref{hyper-param-table} shows the result of hyperparameter tuning for every pipeline.
In the URL pipeline, Adam with the regularization parameter $0.001$ yields the model with the lowest misclassification rate during the initial training phase.
\todo[inline]{results of taxi and criteo hyperparameter tuning come here}
Figre \ref{hyper-param-figure} shows the results of the different hyperparameter configuration on models during the deployment.
To make the deployment figure more readable, we avoid displaying the result of every possible combination of hyperparameters and only show the result of the best configuration for each learning rate adaptation technique.
After the initial training, for every configuration, we deploy the model and use 10 \% of the remaining data to evaluate the model after deployment.
For the URL pipeline, Adam with the regularization parameter $0.001$ also has the best performance during the deployment, followed by Adadelta and Rmsprop.
\todo[inline]{Taxi and Criteo}
This experiment confirms that the hyperparameter configuration that performs the best during the initial stochastic gradient descent training also performs the best for the proactive training.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/parameter-selection-figure}}
\caption{Result of hyperparameter tuning during initial phases of deployment}
\label{hyper-param-figure}
\end{figure}

\textbf{Sampling Methods.}
The proactive training is also affected by the choice of the sampling method.
Each instance of the proactive training updates the deployed model using the provided sample.
Therefore, the quality of the model after an update is directly related to the quality of the data inside the sample.
We evaluate the effect of three different sampling methods, time-based, window-based, and uniform, on the quality of the deployed model.
The time-based sampling method is a weighted sampling approach that assigns higher weights to the recent items. 
The window-based sampling method only samples data from an active window, where the size of the window is 10\% of the total deployment duration (12 days for URL, 2 months for Taxi, and 1.5 days for Criteo).
Finally, uniform sampling method samples data from the entire previously seen data.
For each pipeline, the sampling rate for the proactive training is similar to the sampling rate of the initial stochastic gradient descent optimization(0.1 for all three pipelines).
Figure \ref{sampling-method-figure} shows the effect of different sampling methods on the quality of the model during the deployment phase.
\todo[inline]{I expect similar performance for taxi and criteo, therefore I will not talk about each figure individually}
Time-based sampling achieves the highest quality for all the models.
Every dataset is gathered throughout a long period of time.
Therefore, the underlying characteristics of the data gradually change over time.
A time-based sampling approach is more likely to select the recent items for the proactive training.
As a result, the deployed model is influenced by the more recent items and performs better on the incoming prediction requests.
For the URL pipeline, window-based and uniform sampling have very similar performances.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/sampling-mode-figure}}
\caption{Effect of different sampling methods on quality}
\label{sampling-method-figure}
\end{figure}

%\textbf{Scheduling Policy. }
%In this section, we analyze the scheduling policy of our deployment platform.
%In our prototype, we simulated 2 days of continuous training of Criteo data using Apache Spark.
%Since the streaming component of Apache Spark requires a fixed interval for executing mini batches, we analyze the effect of our scheduling policy analytically.
%
%Figure \ref{fig:scheduling-policy-time} shows the actual execution time of every proactive training throughout the simulation.
%The execution time of the proactive training ranges from $23$ to $53$ seconds.
%In order for the scheduler component to effectively schedule proactive training, it requires the prediction latency, \hl{prediction throughput} \todo[inline]{Is it rate or throughput? Clipper uses the latter}, and a user-defined slack parameter.
%In our estimation, we use a slack parameter of $10$.
%We estimate the throughput and latency based on the time it takes for the deployment platform to predict the labels of the evaluation dataset.
%The evaluation dataset contains 2 million data points.
%The deployment platform is queried using the evaluation dataset every minute and requires $15$ seconds to return the predictions in the worst case scenario (when the evaluation dataset is stored on disk). 
%This amounts to a latency of $7 * 10 ^ {(-6)}$ seconds (7.5 micro seconds) and a throughput of $34,000$ requests per second.
%
%Based on above parameters, the scheduler computes the scheduling intervals for every execution of the proactive training.
%
%\begin{figure}[h!]
%\centering
%\includegraphics[width=\columnwidth]{../images/experiment-results/criteo-scheduling-experiment.eps}
%\caption{Analysis of scheduling policy}
%\label{fig:scheduling-policy-time}
%\vspace{2mm}
%\end{figure}
%
%Using the slack parameter, we can guide the scheduler to increase or decrease the scheduling intervals of the proactive training.
%The slack parameter allows for the deployment platform to accommodate surges in the incoming prediction requests and new training data.
%In scenarios where sudden surges are expected (e.g., online stores), we recommend a large slack parameter (recommended value is $10$). \todo[inline]{No exp. validate this}

\subsection{EX3: Optimizations Effects}
In this experiment, we analyze the effects of the optimizations, live statistics analysis and materialization of features, on the end-to-end deployment time.
Figure \ref{optimization-effect} shows the effect of optimizations on total training time.
By materializing the features, new instances of the proactive training have access directly to the preprocessed data.
As a result, proactive training directly computes the gradient and updates the deployed model.
For the URL pipeline, our optimizations have reduced the continuous training time by 50\%.
\hl{I will update Figure} \ref{optimization-effect} \hl{and show the time for every component of the pipeline and how the optimizations helped in reducing the time.}
\begin{figure}[h!]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/optimization-time-experiment}}
\caption{Effect of the optimizations on total training time in continuous deployment approach}
\label{optimization-effect}
\end{figure}

\subsection{Discussion} \label{subsec:discussion}
Continuous deployment approach can reduce the end-to-end deployment time by 1 order of magnitude.
However, it may reduce the quality as well.
\todo[inline]{Tilmann: I feel this is a bit odd. I would rather do a simpler tradeoff calculation. Behrouz: I think we can just use a side by side figure in experiment 1, where we show the cumulative error and time for every deployment mode. }
\todo[inline]{The formula is a bit complicated now, but we should be able to replace the nominator and denominator by $relative\_err(t)$ and $relative\_cost(t)$. First I'd like to know your opinion on whether this is the best way to show the trade-off between quality and time}
To understand the trade-off between quality and the training time, we define a performance indicator ($PI$) as:\\
\begin{center}
$PI(t) = \dfrac{\dfrac{err_b(t) - err(t)}{err_b(t)}}{\dfrac{cost(t) - cost_b(t)}{cost_b(t)}+ 1}   * 100$
\end{center}
where $err(t)$ and $cost(t)$ are the cumulative error rate and the cumulative deployment cost (training time + prediction time) at time $t$ and  $err_b(t)$ and $cost_b(t)$ are the cumulative error rate and the cumulative deployment cost of the baseline deployment approach (which only includes the prediction time).
The value of $PI$ falls between 0 and 100.
The value of $PI$ for the baseline is always 0 (no increase in quality over the baseline).
A $PI$ of 100 indicates an Oracle deployment approach where the deployed model has an error rate of 0 and deployment cost of similar to the baseline (no training of the model at all).
Bigger values of $PI$ signal the gain in quality has surpassed the loss in the total deployment cost.
Figure \ref{quality-vs-time} shows the value of $PI$ over time for the continuous and periodical deployment approaches.
For the periodical deployment approach, the quality of the models is slightly higher than the continuous deployment approach.
However, due to the extremely long training time, the cumulative cost of the deployment is increasing exponentially with every periodical training (as the size of the historical data doubles for every training).
As a result, the value of $PI$ for the periodical deployment approach decreases as further training (cost of deployment) does not justify the decrease in error rate.
On the other hand, the continuous deployment approac\emph{â€¢}h guarantees high-quality models with only a linear increase in the cumulative deployment cost (the training time for every proactive training is constant) which results in a slow increase in the value of $PI$ over time. 

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/quality-vs-time}}
\caption{Trade-off between model quality and total training time.}
\label{quality-vs-time}
\end{figure}


%\textbf{Side effect of periodical deployment.}
%In our experiments, incoming predictions where paused until the end of a periodical training.
%However, in real-world scenarios, prediction queries arrive constantly arrive at the system.
%Therefore, prediction queries are answered by the model before the periodical training was initiated. 
%Moreover, during the periodical training, the model cannot be updated using online learning algorithm as well.
%As a result, the performance of the model while a new model is being trained does not improve.
%The average training time for the proactive training is very small (200 ms for the URL pipeline \hl{nyc and criteo}).
%Therefore, predictions queries are always answered by an up-to-date model and online learning is not paused in the continuous deployment.


To sum up, our continuous deployment approach for some datasets may result in a small reduction in the quality.
However, the decrease in the total end-to-end deployment time overshadows the loss of the quality as many real-world deployment scenarios require up-to-date models as new data, such as users and ads in the ads prediction use case and new city regions in the taxi travel time prediction use case, becomes available.


