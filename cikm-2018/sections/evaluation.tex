\section{Evaluation} \label{evaluation} 
To evaluate our continuous training approach, we perform several experiments.
The goal of the experiments is to answer the following questions:\\
1. How does our continuous deployment approach perform in comparison to online and periodical deployment approaches with regards to model quality and training time? \\
2. What is the effect of learning rate, regularization, and sample size on the continuous deployment? \\
3. What are the effects of live statistics analysis and materialization of preprocessed features on the training time?

by simulating the deployment of machine learning pipelines using different real-world datasets.

\subsection{Setup}\label{subsec:setup}
\textbf{Pipelines.}
We design three pipelines for all the experiments.\\
\textit{URL pipeline. } The URL pipeline consists of 5 components: input parser, missing value imputer, standard scaler, feature hasher, and an SVM model.
We use the URL pipeline to process the URL dataset for classifying URLs, gathered over a 120 days period, into malicious and legitimate groups \cite{ma2009identifying}.\\
\textit{Criteo Pipeline.} 
The Criteo pipeline consists of 5 components: input parser, missing value imputer, standard scaler, feature hasher, and a Logistic Regression model.
The Criteo pipeline process the Terabyte Criteo click log dataset is used for benchmarking algorithms for clickthrough rate (CTR) prediction \cite{criteo-log}.\\
\textit{\hl{Probably} NewYork Taxi.}
The Criteo pipeline consists of 4 components: input parser, missing value imputer, feature extractor, standard scaler, and a Linear Regression model.

\textbf{Deployment Environment. }
We deploy the URL pipeline on a single laptop running a macOS with 2,2 GHz intel Core i7 and 16 GB of RAM and the Criteo pipeline on a cluster of 21 machines (intel Xeon 2.4 GHz 16 cores, 28 of dedicated RAM per node).
Both environments are using Apache Spark 2.2.0 as their execution engine.

\textbf{Deployment Process. }
For both the URL and the Criteo pipeline we first trained a model on the first day of the data.
We then deploy the model (and the pipeline) and use the rest of the data for reporting the prediction quality and further training.
The URL and the Criteo datasets do not have timestamps, therefore we divide every day into micro batches of roughly 1 minute of data. 
Each micro batch is sequentially sent to the deployment environment where first they are used for reporting the quality and then used for training model.

\subsection{EX1: Deployment Modes}
In this experiment, we investigate the effect of our continuous deployment approach on model quality and total training time.
We use 4 different deployment approaches.
\begin{itemize}
\item Online: purely online learning
\item Baseline: no training at all (only the initial model used)
\item Periodical: periodically retraining the model and pipeline
\item Continuous: our approach
\end{itemize}

\textbf{Model Quality. }
Figure \ref{deployment-quality-figure} shows the quality of different deployment methods for each pipeline.

\begin{figure}[h!]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/deployment-quality-experiment}}
\caption{Model Quality for different deployment approaches}
\label{deployment-quality-figure}
\end{figure}

\textbf{Total Training Time. }
Figure \ref{deployment-time-figure} shows the total training time for each deployment approach.

\begin{figure}[h!]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/deployment-time-experiment}}
\caption{End-to-end deployment time for different deployment approaches}
\label{deployment-time-figure}
\end{figure}

\subsection{EX2: System Tuning}
In this experiment, we investigate the effect of different parameters on the quality of the models after deployment (answering the second question).

\textbf{Optimization algorithm parameters. }
Stochastic gradient descent is heavily dependent on the choice of learning rate, learning rate adaptation technique, and regularization parameter (called hyperparameters). 
To find the best hyperparameters, we perform a grid search and for each configuration train over the initial model and evaluate model over the second day.
We then show that using the configuration that yields the lowest generalization error during the initial training will also result in the best set of hyperparameters for our continuous training approach.

Table \ref{hyper-param-table} shows the result of performing hyperparameter tuning.
\begin{table*}[!h]
\centering
\begin{adjustbox}{max width=\textwidth}
\input{../images/experiment-results/tikz/parameter-selection-table}
\end{adjustbox}
\caption{Hyper parameter tuning during initial training}
\label{hyper-param-table}
\end{table*}

Figure \ref{hyper-param-figure} shows the quality of deployed pipelines trained using the best set of hyperparameters. 
For each learning rate adaptation technique, we pick the learning rate and regularization parameter that yields the lowest generalization error.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/parameter-selection-figure}}
\caption{Hyperparameter tuning during first days of deployment}
\label{hyper-param-figure}
\end{figure}

\textbf{Sampling Methods. }
Figure \ref{sampling-method-figure} shows the effect of different sampling methods on the quality.
\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/sampling-mode-figure}}
\caption{Effect different sampling methods on quality}
\label{sampling-method-figure}
\end{figure}

\textbf{Scheduling Policy. }
In this section, we analyze the scheduling policy of our deployment platform.
In our prototype, we simulated 2 days of continuous training of Criteo data using Apache Spark.
Since the streaming component of Apache Spark requires a fixed interval for executing mini batches, we analyze the effect of our scheduling policy analytically.

Figure \ref{fig:scheduling-policy-time} shows the actual execution time of every proactive training throughout the simulation.
The execution time of the proactive training ranges from $23$ to $53$ seconds.
In order for the scheduler component to effectively schedule proactive training, it requires the prediction latency, \hl{prediction throughput} \todo[inline]{Is it rate or throughput? Clipper uses the latter}, and a user-defined slack parameter.
In our estimation, we use a slack parameter of $10$.
We estimate the throughput and latency based on the time it takes for the deployment platform to predict the labels of the evaluation dataset.
The evaluation dataset contains 2 million data points.
The deployment platform is queried using the evaluation dataset every minute and requires $15$ seconds to return the predictions in the worst case scenario (when the evaluation dataset is stored on disk). 
This amounts to a latency of $7 * 10 ^ {(-6)}$ seconds (7.5 micro seconds) and a throughput of $34,000$ requests per second.

Based on above parameters, the scheduler computes the scheduling intervals for every execution of the proactive training.

\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{../images/experiment-results/criteo-scheduling-experiment.eps}
\caption{Analysis of scheduling policy}
\label{fig:scheduling-policy-time}
\vspace{2mm}
\end{figure}

Using the slack parameter, we can guide the scheduler to increase or decrease the scheduling intervals of the proactive training.
The slack parameter allows for the deployment platform to accommodate surges in the incoming prediction requests and new training data.
In scenarios where sudden surges are expected (e.g., online stores), we recommend a large slack parameter (recommended value is $10$). \todo[inline]{No exp. validate this}

\subsection{EX3: Optimizations Effects}
In this experiment, we analyze the effects of the two optimizations, live statistics analysis and materialization of preprocess features, on the total training time.
Figure \ref{optimization-effect} shows the effect of optimizations on total training time.
\begin{figure}[h!]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/optimization-time-experiment}}
\caption{Effect of the optimizations on total training time in continuous deployment approach}
\label{optimization-effect}
\end{figure}

\subsection{Discussion} \label{subsec:discussion}
Our experiments show that our continuous training approach outperforms periodical training of deployed models and pipelines.
By using proactive training we manage to reduce the average error rate by $1.6\%$.
The frequent updates that the continuous training approach applies to the deployed model is the main reason for the reduction in error rate.
As the amount of the existing data increases after each time interval, periodical training requires more training iterations and more advanced techniques to train a model with an acceptable error rate.
Continuous training does not face the same issue and the error rate of the model decreases when using the continuous training approach, as demonstrated by the experiments.
The continuous training approach enables the model to adapt to the recently unseen data faster and allows the model to be updated with new features.
In contrast to the continuous training, in periodical training, new features are only discovered after each training interval (e.g., 1 day for the Criteo pipeline).
As a result, the deployment platform discards the newly available features when answering prediction requests until the next training.

The continuous training approach reduces the total training time by a factor of $5$ after two days of training.
Moreover, the online statistics computation and data materialization optimizations reduce the total training time by 2 orders of magnitude over the state-of-the-art deployment approaches.
After the model is deployed, the training time for the continuous training approach stays constant as the frequency of proactive training remains the same.
However, periodical training has to process larger datasets at the end of each time interval which leads to the large difference in total training time between continuous and periodical training.

Proactive training is an extension of the SGD optimization algorithm, where during each scheduled training an optional sample of the historical data and the newly available training data are combined and used to train the deployed model in-place.
Therefore, tuning proactive training is similar to the process of tuning the SGD algorithm.
In our experiments, we show that the learning rate adaptation technique that works best with SGD during the offline training also results in the lowest error rate when used in the proactive training.

We demonstrate how different sampling approaches (simple random sampling, time-based sampling, and no sampling) affect the quality of the model.
To perform efficient time-based sampling, the data manager uses a partitioning technique that stores the incoming data in partitions and assigns timestamps to each partition.
Our experiments show that using the data partitioning technique, we can effectively provide samples from different time intervals without incurring an overhead on the deployment platform.
However, while the sampling operations themselves do not incur any overhead, the extra amount of data that is generated as the result of the sampling increases the proactive training time.
To alleviate the issue, we propose a scheduling policy that dynamically adapts to both the rate of the incoming prediction requests and the time required for executing a proactive training.
We show that our scheduling policy can effectively execute the proactive training and adapt to the changes in the rate of the incoming data, the prediction latency, the proactive training time, and the sudden data surges.





