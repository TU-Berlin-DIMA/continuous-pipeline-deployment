\section{Evaluation} \label{evaluation} 
To evaluate the performance of our deployment platform, we perform several experiments.
Our main goal is to show that the continuous deployment approach maintains the quality of the deployed model while reducing the total training time.
Specifically, we answer the following questions:\\
1. How does our continuous deployment approach perform in comparison to online and periodical deployment approaches with regards to model quality and training time? \\
2. What are the effects of the learning rate adaptation method, the regularization parameter, and the sampling strategy on the continuous deployment? \\
3. What are the effects of live statistics analysis and materialization of preprocessed features on the training time?

To that end, first, we design two pipelines each processing one real-world dataset.
Then, we deploy the pipelines using different deployment approaches.

\subsection{Setup}\label{subsec:setup}
\textbf{Pipelines.}
We design two pipelines for all the experiments.

\textit{URL pipeline.} We use the URL pipeline processes the URL dataset for classifying URLs, gathered over a 121 days period, into malicious and legitimate groups \cite{ma2009identifying}.\\
The pipeline consists of 5 components: input parser, missing value imputer, standard scaler, feature hasher, and an SVM model.
To evaluate the SVM model, we compute the misclassification rate on the unseen data.

\textit{Taxi Pipeline.} The Taxi pipeline processes the Newyork taxi trip dataset and predicts the trip duration of every taxi ride \cite{newyork-taxi}. 
The pipeline consists of 5 components: input parser, feature extractor, anomaly detector, standard scaler, and a Linear Regression model.
The Taxi pipeline is more complex than the URL pipeline. 
We design the pipeline based on the solutions of the top scorers of the New York City Taxi Trip Duration Kaggle competition\footnote{https://www.kaggle.com/c/nyc-taxi-trip-duration/}. 
The input parser computes the actual trip duration by first extracting the pickup and drop off time fields from the input records and calculating the difference (in seconds) between the two values.
The feature extractor computes the haversine distance\footnote{https://en.wikipedia.org/wiki/Haversine\_formula}, the bearing\footnote{https://en.wikipedia.org/wiki/Bearing\_(navigation)}, the hour of the day, and the day of the week from the input records. 
Finally, the anomaly detector filters the trips that are longer than 22 hours, smaller than 10 seconds, or the trips that have a total distance of zero (the car never moved).
To evaluate the model, we use the Root Mean Squared Logarithmic Error (RMSLE) measure.
RMSLE is also the chosen error metric for the NYC taxi Kaggle competition.

\textbf{Deployment Environment. }
\todo[inline]{Because the URL data is small, the run time on cluster is larger because of the hdfs overhead. Should I include the results from the cluster or a single node? In cluster total time of continuous is 7 times less than periodical, but in local it is 13 times better.}
We deploy the URL pipeline on a single laptop running a macOS with 2,2 GHz Intel Core i7 and 16 GB of RAM and the Taxi pipeline on a cluster of 21 machines (Intel Xeon 2.4 GHz 16 cores, 28 GB of dedicated RAM per node).
In our current prototype, we are using Apache Spark as the execution engine.
The data manager component utilizes the Hadoop Distributed File System (HDFS) for storing the historical data \cite{shvachko2010hadoop}.
We leverage some of the components of the machine learning library in Spark to implement the proactive trainer.
Moreover, we utilize the caching mechanism of the Apache Spark for materializing the transformed features.


\textbf{Deployment Process. }
Table \ref{dataset-description} describes the details of the datasets, the amount of data for the initial training, and the amount of data for prediction requests and further training after deployment. 
For the URL pipeline, we first train a model on the first day of the data (day 0).
For the Taxi pipeline, we train a model using the data from January 2015.
For both datasets, since the entire data fits in the memory of the computing nodes, we use batch gradient descent (sampling ratio of 1.0) during the initial training.
We then deploy the models (and the pipelines).
We use the remaining data for sending prediction queries and further training of the deployed models.
Using the prediction queries, we compute the cumulative prequential error rate over time to evaluate the performance of each model during the deployment \cite{dawid1984present}.
The URL dataset does not have timestamps. 
Therefore, we divide every day of the data into micro-batches of 1 minute which results in a 12000 micro-batches of data.
We sequentially send the micro-batches to the deployment platform.
The deployment platform first uses the micro-batch for prequential evaluation and then updates the deployed model.
The NYC taxi dataset includes timestamps. 
In our experiments, each micro-batch of the NYC taxi dataset contains one hour of the data. 
The micro-batches are sent in order of the timestamps (from 2015-Feb-01  00:00 to 2016-Jun-31 24:00, an 18 months period) to the deployment platform.

\begin{table}[h!]
\centering
\begin{tabular}{lrrll}
\hline
\textbf{Dataset}  & \textbf{size} &\textbf{\# instances} & \textbf{Initial} & \textbf{Deployment} \\
\hline
URL        &  4.2 GB 	& 2.4 M  			& Day 0        	  & Day 1-120          \\
Taxi        &  84 GB 	    & 280 M            & Jan15              & Feb15 to Jun16    \\
%Criteo       &  & XX B            & Day 0           & Day 1-10       \\
\hline
\end{tabular}
\caption{Description of Datasets. The \textbf{Initial} column indicates the amount of data used during the initial model training and the \textbf{Deployment} column indicates the amount of data used for both prequential evaluation and further model training}  
\label{dataset-description}
\end{table}


\subsection{EX1: Deployment Modes}
In this experiment, we investigate the effect of our continuous deployment approach on model quality and the total training time.
We use 3 different deployment approaches.
\todo[inline]{Should we mention Static deployment (no training after deployment)? We cannot show the quality result since error rate is too high, and other approaches are not visible.}
\begin{itemize}
\item Online: deploy the pipeline and only use online (gradient descent) method for updating the model
\item Periodical: deploy the pipeline and periodically train new models 
\item Continuous: deploy the pipeline and continuously update the model using our platform
\end{itemize}

The periodical deployment initiates a full retraining every 10 days and every month for URL and Taxi pipeline, respectively.
To improve the performance of the periodical deployment, we utilize the warm starting technique, used in the TFX framework \cite{baylor2017tfx}.
In warm starting, each periodical training uses the existing parameters such as the pipeline statistics (e.g., standard scaler), model weights, and learning rate adaptation parameters (e.g., the average of past gradients used in Adadelta, Adam, and Rmsprop) when training new models.

\textbf{Model Quality. }
Figure \ref{deployment-quality-figure} (a) and (c) show the cumulative error rate over time for the different deployment approaches.
For both datasets, the continuous and the periodical deployment result in a lower error rate than the online deployment.
Online deployment visits every incoming training data point only once.
As a result, the model updates are more prone to noise.
This results in a higher error rate than the continuous and periodical deployment.
In Figure \ref{deployment-quality-figure} (a), for the URL dataset, during the first 110 days of the deployment, the continuous approach has a lower error rate than the periodical.
Only after the final retraining, the periodical deployment slightly outperforms the continuous deployment.
However, the start to the end of the deployment process, the continuous deployment improves the average error rate by 0.3\% and 1.5\% over the periodical and online deployment.
In Figure \ref{deployment-quality-figure}  (c), for the Taxi dataset, the continuous deployment always attains a smaller error rate than the periodical deployment.
Overall, the continuous deployment improves the error rate by 0.05\% and  0.1\% over the periodical and online deployment, respectively.

When compared to the online deployment, periodical deployment slightly decreases the error rate after every training.
However, between every retraining, the platform updates the model using online learning.
This contributes to the higher error rate than the continuous deployment, where the platform continuously trains the deployed model using the historical data.

\begin{figure*}[t]
\centering
\resizebox{\textwidth}{!}{\input{../images/experiment-results/tikz/deployment-quality-and-time-experiment}}
\caption{Model Quality and Training cost for different deployment approaches}
\label{deployment-quality-figure}
\end{figure*}

In Figure \ref{deployment-quality-figure} (b) and (d), we report the cumulative cost over time for every deployment approach.
We define the deployment cost as the total time spent in data preprocessing, model training, and performing prediction.
For the URL dataset (Figure \ref{deployment-quality-figure} (b)), online deployment has the smallest cost (around 34 minutes) as it only scans each data point once (around 2.3 million scans).  
The continuous deployment approach scans 45 million data points.
However, the total cost at the end of the deployment is only two times larger than the online deployment approach (around 65 minutes).  
Because of the online statistics computation and the feature materialization optimization, a large part of the data processing time is avoided.
For the periodical deployment approach, the cumulative deployment cost starts similar to the online deployment approach.
However, after every offline retraining, the deployment cost substantially increases.
At the end of the deployment process, the total cost for the periodical deployment is more than 850 minutes, 13 times more than the total cost of the continuous deployment approach.
Each data point in the URL dataset has more than 3 million features.
Therefore, the convergence time for each retraining is very high.
The high data-dimensionality and repeated data preprocessing contribute to the large deployment cost of the periodical deployment.

For the Taxi dataset (Figure \ref{deployment-quality-figure} (d)), the cost of online, continuous, and periodical deployment are 250, 340, and 1600 minutes, respectively.
Similar to the URL dataset, continuous deployment only adds a small overhead to the deployment cost when compared with the online deployment.
Contrary to the URL dataset, the feature size of the Taxi dataset is 11.
Therefore, offline retraining converges faster to a solution.
Therefore, for the Taxi dataset, the cost of the periodical deployment is 5 times larger than the continuous deployment (instead of 13 times for URL dataset). 

\subsection{EX2: System Tuning}
\begin{table*}[t]
\centering
\begin{adjustbox}{max width=\textwidth}
\input{../images/experiment-results/tikz/parameter-selection-table}
\end{adjustbox}
\caption{Hyperparameter tuning during initial training (bold numbers show the best result for a dataset)}
\label{hyper-param-table}
\end{table*}

In this experiment, we investigate the effect of different parameters on the quality of the models after deployment.
As described in Section \ref{proactive-training}, proactive training is an extension of the stochastic gradient descent to the deployment phase.
Therefore, we expect the set of hyperparameters with best performance during the initial training also performs the best during the deployment phase.

\textbf{Proactive Training Parameters. }
Stochastic gradient descent is heavily dependent on the choice of learning rate and the regularization parameter.
To find the best set of hyperparameters for the initial training, we perform a grid search.
We use advanced learning rate adaptation techniques (Adam, Adadelta, and RMSPROP) for both initial and proactive training.

For each dataset, we divide the initial data (from Table \ref{dataset-description}) into a training and evaluation set.
For each configuration, we first train a model using the training set and then evaluate the model using the evaluation set.
Table \ref{hyper-param-table} shows the result of hyperparameter tuning for every pipeline.
For URL dataset, Adam with regularization parameter $1E{\text -}3$ yields the model with lowest error rate.
The Taxi dataset is less complex than the URL dataset and has a smaller number of feature dimensions.
As a result, the choice of different hyperparameter does not have a large impact on the quality of the model.
The Rmsprop adaptation technique with regularization parameter of $1E{\text -}4$ results in a slightly better model than the other configurations.

After the initial training, for every configuration, we deploy the model and use 10 \% of the remaining data to evaluate the model after deployment.
Figure \ref{hyper-param-figure} shows the results of the different hyperparameter configurations on the deployed model.
To make the deployment figure more readable, we avoid displaying the result of every possible combination of hyperparameters and only show the result of the best configuration for each learning rate adaptation technique.
For the URL dataset, similar to the initial training, Adam with regularization parameter $1E{\text -}3$ results in the best model.
For the Taxi dataset, we observe similar behaviour to the initial training.
Different configurations do not have a significant impact on the quality of the deployed model.

This experiment confirms that the effect of the hyperparameters (learning rate and regularization) during the initial and proactive training are the same.
Therefore, we tune the parameters of the proactive training based on the result of the hyperparameter search during the initial training.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/parameter-selection-figure}}
\caption{Result of hyperparameter tuning during initial phases of deployment}
\label{hyper-param-figure}
\end{figure}

\textbf{Sampling Methods.}
The choice of the sampling strategy also affects the proactive training.
Each instance of the proactive training updates the deployed model using the provided sample.
Therefore, the quality of the model after an update is directly related to the quality of the data inside the sample.
We evaluate the effect of three different sampling methods, time-based, window-based, and uniform, on the quality of the deployed model.
The sample size is similar to the sample size during the initial training ($16k$ and $1M$ for URL and Taxi data, respectively).
Figure \ref{sampling-method-figure} shows the effect of different sampling strategies on the quality of the deployed model.
For the URL dataset, time-based sampling achieves the lowest error rate for the deployed model.
For the URL dataset, the underlying characteristics of the data gradually change over time, as new features are added to the dataset \cite{ma2009identifying}.
A time-based sampling approach is more likely to select the recent items for the proactive training.
As a result, the deployed model performs better on the incoming prediction queries.
Taxi dataset does not contain any known concept drift.
As a result, we observe that different sampling strategies have the same effect on the quality of the deployed model.

Since the existence of concept drift is not always known beforehand, we choose time-based sampling for our deployment platform.
For datasets that contain concept drift, time-based sampling outperforms other sampling strategies, while providing the same performance for datasets without concept drift.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/sampling-mode-figure}}
\caption{Effect of different sampling methods on quality}
\label{sampling-method-figure}
\end{figure}

%\textbf{Scheduling Policy. }
%In this section, we analyze the scheduling policy of our deployment platform.
%In our prototype, we simulated 2 days of continuous training of Criteo data using Apache Spark.
%Since the streaming component of Apache Spark requires a fixed interval for executing mini batches, we analyze the effect of our scheduling policy analytically.
%
%Figure \ref{fig:scheduling-policy-time} shows the actual execution time of every proactive training throughout the simulation.
%The execution time of the proactive training ranges from $23$ to $53$ seconds.
%In order for the scheduler component to effectively schedule proactive training, it requires the prediction latency, \hl{prediction throughput} \todo[inline]{Is it rate or throughput? Clipper uses the latter}, and a user-defined slack parameter.
%In our estimation, we use a slack parameter of $10$.
%We estimate the throughput and latency based on the time it takes for the deployment platform to predict the labels of the evaluation dataset.
%The evaluation dataset contains 2 million data points.
%The deployment platform is queried using the evaluation dataset every minute and requires $15$ seconds to return the predictions in the worst case scenario (when the evaluation dataset is stored on disk). 
%This amounts to a latency of $7 * 10 ^ {(-6)}$ seconds (7.5 micro seconds) and a throughput of $34,000$ requests per second.
%
%Based on above parameters, the scheduler computes the scheduling intervals for every execution of the proactive training.
%
%\begin{figure}[h!]
%\centering
%\includegraphics[width=\columnwidth]{../images/experiment-results/criteo-scheduling-experiment.eps}
%\caption{Analysis of scheduling policy}
%\label{fig:scheduling-policy-time}
%\vspace{2mm}
%\end{figure}
%
%Using the slack parameter, we can guide the scheduler to increase or decrease the scheduling intervals of the proactive training.
%The slack parameter allows for the deployment platform to accommodate surges in the incoming prediction requests and new training data.
%In scenarios where sudden surges are expected (e.g., online stores), we recommend a large slack parameter (recommended value is $10$). \todo[inline]{No exp. validate this}

\subsection{EX3: Optimizations Effects}
In this experiment, we analyze the effects of the optimizations, namely, online statistics computation and materialization of features, on the deployment cost (total data preprocessing, and model training time).
Figure \ref{optimization-effect} shows the effect of optimizations on deployment cost.
Online statistics computation and feature materialization eliminate the need for data preprocessing.
As a result, proactive training computes the gradient and updates the deployed model without the need for data preprocessing.
Our optimizations improve the deployment cost by 70\% and 130\% for the URL and Taxi dataset, respectively.
The Taxi pipeline is more complex and contains more components than the URL pipeline.
As a result, the reduction in the Taxi dataset is greater than the URL dataset.

\begin{figure}[h!]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/optimization-time-experiment}}
\caption{Effect of the optimizations on total training time in continuous deployment approach}
\label{optimization-effect}
\end{figure}

\subsection{Discussion} \label{subsec:discussion}
\textbf{Trade off between quality and training cost.}
In many of the real-world use cases, even a small improvement in the quality of a deployed model has an important impact.
Therefore, one can employ more complex pipelines and machine learning models to train better models.
However, during the deployment where prediction queries and training data becomes available at a high rate, one must consider the effect of the training time.
To ensure the model is always up-to-date, the platform must constantly update the model.
Long retraining time has a negative impact on the prediction accuracy as deployed model quickly becomes stale.
Figure \ref{trade-off-figure} shows the trade-off between the quality and total cost of the deployment.

By utilizing continuous deployment, we improve the quality by 0.05\% and 0.3\% for the Taxi and URL datasets over the periodical deployment approach.
We also reduce the cost of the deployment 5 to 13 times when compared with periodical deployment.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/quality-vs-time}}
\caption{Trade off between average quality of the deployed model and deployment cost}
\label{trade-off-figure}
\end{figure}

\textbf{Side effect of periodical deployment.}
In our experiments, when the deployment platform is retraining the model, we pause the inflow of the prediction queries. 
However, in real-world scenarios, prediction queries constantly arrive at the system.
Therefore, deployment platform answers the prediction queries using the deployed model before the retraining began.
As a result, the error rate of the model during the retraining increases. However, the average time for the proactive training is small (200 ms for the URL dataset and 700 ms for the Taxi dataset).
Therefore, the deployment platform always answers the predictions queries with an up-to-date model.


