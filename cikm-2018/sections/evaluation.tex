\section{Evaluation} \label{evaluation} 
To evaluate the performance of our deployment platform, we perform several experiments.
The goal of the experiments is to answer the following questions:\\
1. How does our continuous deployment approach perform in comparison to online and periodical deployment approaches with regards to model quality and training time? \\
2. What are the effects of the learning rate adaptation method, the regularization parameter, and the sampling strategy on the continuous deployment? \\
3. What are the effects of live statistics analysis and materialization of preprocessed features on the training time?

To that end, first, we design two pipelines each processing one real-world dataset.
Then, we deploy the pipelines using different deployment approaches.

\subsection{Setup}\label{subsec:setup}
\textbf{Pipelines.}
We design two pipelines for all the experiments.

\textit{URL pipeline.} The URL pipeline consists of 5 components: input parser, missing value imputer, standard scaler, feature hasher, and an SVM model.
We use the URL pipeline to process the URL dataset for classifying URLs, gathered over a 121 days period, into malicious and legitimate groups \cite{ma2009identifying}.\\
To evaluate the SVM model, we compute the misclassification rate on the unseen data.

\textit{Taxi Pipeline.}
The Taxi pipeline consists of 5 components: input parser, feature extractor, anomaly detector, standard scaler, and a Linear Regression model.
The Taxi pipeline processes the Newyork taxi trip dataset and predicts the trip duration of every taxi ride \cite{newyork-taxi}. 
The Taxi pipeline is more complex than the URL pipeline. 
We design the pipeline based on the solutions of the top scorers of the New York City Taxi Trip Duration Kaggle competition\footnote{https://www.kaggle.com/c/nyc-taxi-trip-duration/}. 
The input parser computes the actual trip duration by first extracting the pickup and drop off time fields from the input records and calculating the difference (in seconds) between the two values.
The feature extractor computes the haversine distance\footnote{https://en.wikipedia.org/wiki/Haversine\_formula}, the bearing\footnote{https://en.wikipedia.org/wiki/Bearing\_(navigation)}, the hour of the day, and the day of the week from the input records. 
Finally, the anomaly detector filters the trips that are longer than 22 hours, smaller than 10 seconds, or the trips that have a total distance of zero (the car never moved).
To evaluate the model, we use the Root Mean Squared Logarithmic Error (RMSLE) measure.
RMSLE is also the chosen error metric for the NYC taxi Kaggle competition.

\textbf{Deployment Environment. }
\todo[inline]{Because the URL data is small, the run time on cluster is larger because of the hdfs overhead. Should I include the results from the cluster or a single node?}
We deploy the URL pipeline on a single laptop running a macOS with 2,2 GHz Intel Core i7 and 16 GB of RAM and the Taxi pipeline on a cluster of 21 machines (Intel Xeon 2.4 GHz 16 cores, 28 GB of dedicated RAM per node).
Both environments are using Apache Spark 2.2.0 as execution engine.

\textbf{Deployment Process. }
Table \ref{dataset-description} describes the details of the datasets, the amount of data for the initial training, and the amount of data for prediction requests and further training after deployment. 
For the URL pipeline, we first train a model on the first day of the data (day 0).
For the Taxi pipeline, we train a model using the data from January 2015.
We then deploy the models (and the pipelines).
We use the remaining data for sending prediction queries and further training of the deployed models.
Using the prediction queries, we compute the cumulative prequential error rate over time to evaluate the performance of each model during the deployment \cite{dawid1984present}.
The URL dataset does not have timestamps. 
Therefore, We divide every day of the data into micro-batches of 1 minute which results in a 12000 micro-batches of data.
We sequentially send the micro-batches to the deployment platform.
The deployment platform first uses the micro-batch for prequential evaluation and then updates the deployed model.
The NYC taxi dataset includes timestamps. 
In our experiments, each micro-batch of the NYC taxi dataset contains one hour of the data. 
The micro-batches are sent in order of the timestamps (from 2015-Feb-01  00:00 to 2016-Jun-31 24:00, an 18 months period) to the deployment platform.

\todo[inline]{number of datapoints for taxi and size of the data after pipeline processing?}
\begin{table}[h!]
\centering
\begin{tabular}{lrrll}
\hline
\textbf{Dataset}  & \textbf{size} &\textbf{\# instances} & \textbf{Initial} & \textbf{Deployment} \\
\hline
URL        &  2.21 GB 	& 2.3 M  			& Day 0        	  & Day 1-120          \\
Taxi        &  30.44 GB 	& XX B              & Jan15              & Feb15 to Jun16    \\
%Criteo       &  & XX B            & Day 0           & Day 1-10       \\
\hline
\end{tabular}
\caption{Description of Datasets. The \textbf{Initial} column indicates the amount of data used during the initial model training and the \textbf{Deployment} column indicates the amount of data used for both prequential evaluation and further model training}  
\label{dataset-description}
\end{table}


\subsection{EX1: Deployment Modes}
In this experiment, we investigate the effect of our continuous deployment approach on model quality and the total training time.
We use 4 different deployment approaches.
\todo[inline]{If we have space, I will add the result of the static (no training after deployment) as well}
\begin{itemize}
\item Online: deploy the pipeline and only use online (gradient descent) method for updating the model
\item Periodical: deploy the pipeline and periodically train new models 
\item Continuous: deploy the pipeline and continuously update the model using our framework
\end{itemize}

To improve the performance of the periodical deployment, we utilize the warm starting technique, used in the TFX framework \cite{baylor2017tfx}.
In warm starting, each periodical training uses the existing parameters such as the pipeline statistics (e.g., standard scaler), model weights, and learning rate adaptation parameters (e.g., the average of past gradients used in Adadelta, Adam, and Rmsprop) when training new models.

\textbf{Model Quality. }
Figure \ref{deployment-quality-figure} shows the cumulative error rate and deployment cost over time for the different deployment approaches.
The periodical deployment initiates a full retraining every 10 days and every month for URL and Taxi pipeline respectively.

For both pipelines, the continuous and the periodical deployment result in a lower error rate than the online deployment.
Online deployment visits every incoming training data point once.
As a result, the model updates are noisy which slows down the convergence.

For the URL data, during the first 110 days of the deployment, the continuous approach results in a lower error rate than the periodical.
Only after the final retraining, the periodical deployment manages to slightly outperform the continuous deployment.
From the beginning to the end of the deployment process, the continuous deployment improves the average quality by 1.5\% and 0.3\% over online and periodical deployment approach for the URL pipeline.

\begin{figure*}[t]
\centering
\resizebox{\textwidth}{!}{\input{../images/experiment-results/tikz/deployment-quality-and-time-experiment}}
\caption{Model Quality and Training cost for different deployment approaches}
\label{deployment-quality-figure}
\end{figure*}

The deployment cost is defined as the total time spent in data preprocessing, model training, and performing prediction.
In Figure \ref{deployment-quality-figure}, we report the cumulative cost over time for every deployment approach.
For the URL pipeline, online deployment has the smallest cost (around 34 minutes) as it only scans each data point once (around 2.3 million scans).  
The the continuous deployment approach scans 45 million data points.
However, the total cost at the end of the deployment is only two times larger than the online deployment approach (around 65 minutes).  
Because of the online statistics computation and the feature materialization optimization, a large part of the data processing time is avoided.
For the periodical deployment approach, the cumulative deployment cost starts similar to the online deployment approach.
However, after every offline retraining, the deployment cost substantially increases.
At the end of the deployment process, the total cost for the periodical deployment is more than 850 minutes, 13 times more than the total cost of the continuous deployment approach.
The overhead of repeated data preprocessing and the model retraining contributes to the very large deployment cost of the periodical deployment approach.


\subsection{EX2: System Tuning}
\todo[inline]{Tilmann: What exactly do you wan to show with this? Seems to me that you could summarize the result rather than showing the full experiment since you are just trying out different existing algorithms...}
\begin{table*}[t]
\centering
\begin{adjustbox}{max width=\textwidth}
\input{../images/experiment-results/tikz/parameter-selection-table}
\end{adjustbox}
\caption{Hyperparameter tuning during initial training (bold numbers show the best result for a dataset)}
\label{hyper-param-table}
\end{table*}

In this experiment, we investigate the effect of different parameters on the quality of the models after deployment.
We leverage the well-established hyperparameter tuning technique in the machine learning literature to tune the parameters of proactive training.
As described in Section \ref{proactive-training}, proactive training is an extension of the stochastic gradient descent to the deployment phase.
Therefore, we expect the set of hyperparameters with best performance during the initial training also performs the best during the deployment phase.
Next, we investigate the effect of different sampling techniques on the quality of the deployed model.

\textbf{Proactive Training Parameters. }
Stochastic gradient descent is heavily dependent on the choice of learning rate and the regularization parameter.
To find the best set of hyperparameters for the initial training, we perform a grid search.
Instead of using a constant learning rate, we use advanced learning rate adaptation techniques (Adam, Adadelta, and RMSPROP).
Therefore, during our hyperparameter search process, instead of trying different learning rate values, we search for the best adaptation technique for every dataset.
For each dataset, we divide the initial data (from Table \ref{dataset-description}) into a training and evaluation set.
For each configuration, we first train a model using the training set and then evaluate the model using the evaluation set.
Table \ref{hyper-param-table} shows the result of hyperparameter tuning for every pipeline.
In the URL pipeline, Adam with the regularization parameter $0.001$ yields the model with the lowest misclassification rate during the initial training phase.
\todo[inline]{results of taxi and criteo hyperparameter tuning come here}
Figre \ref{hyper-param-figure} shows the results of the different hyperparameter configuration on models during the deployment.
To make the deployment figure more readable, we avoid displaying the result of every possible combination of hyperparameters and only show the result of the best configuration for each learning rate adaptation technique.
After the initial training, for every configuration, we deploy the model and use 10 \% of the remaining data to evaluate the model after deployment.
For the URL pipeline, Adam with the regularization parameter $0.001$ also has the best performance during the deployment, followed by Adadelta and Rmsprop.
\todo[inline]{Taxi and Criteo}
This experiment confirms that the hyperparameter configuration that performs the best during the initial stochastic gradient descent training also performs the best for the proactive training.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/parameter-selection-figure}}
\caption{Result of hyperparameter tuning during initial phases of deployment}
\label{hyper-param-figure}
\end{figure}

\textbf{Sampling Methods.}
The proactive training is also affected by the choice of the sampling method.
Each instance of the proactive training updates the deployed model using the provided sample.
Therefore, the quality of the model after an update is directly related to the quality of the data inside the sample.
We evaluate the effect of three different sampling methods, time-based, window-based, and uniform, on the quality of the deployed model.
The time-based sampling method is a weighted sampling approach that assigns higher weights to the recent items. 
The window-based sampling method only samples data from an active window, where the size of the window is 10\% of the total deployment duration (12 days for URL, 2 months for Taxi, and 1.5 days for Criteo).
Finally, uniform sampling method samples data from the entire previously seen data.
For each pipeline, the sampling rate for the proactive training is similar to the sampling rate of the initial stochastic gradient descent optimization(0.1 for all three pipelines).
Figure \ref{sampling-method-figure} shows the effect of different sampling methods on the quality of the model during the deployment phase.
\todo[inline]{I expect similar performance for taxi and criteo, therefore I will not talk about each figure individually}
Time-based sampling achieves the highest quality for all the models.
Every dataset is gathered throughout a long period of time.
Therefore, the underlying characteristics of the data gradually change over time.
A time-based sampling approach is more likely to select the recent items for the proactive training.
As a result, the deployed model is influenced by the more recent items and performs better on the incoming prediction requests.
For the URL pipeline, window-based and uniform sampling have very similar performances.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/sampling-mode-figure}}
\caption{Effect of different sampling methods on quality}
\label{sampling-method-figure}
\end{figure}

%\textbf{Scheduling Policy. }
%In this section, we analyze the scheduling policy of our deployment platform.
%In our prototype, we simulated 2 days of continuous training of Criteo data using Apache Spark.
%Since the streaming component of Apache Spark requires a fixed interval for executing mini batches, we analyze the effect of our scheduling policy analytically.
%
%Figure \ref{fig:scheduling-policy-time} shows the actual execution time of every proactive training throughout the simulation.
%The execution time of the proactive training ranges from $23$ to $53$ seconds.
%In order for the scheduler component to effectively schedule proactive training, it requires the prediction latency, \hl{prediction throughput} \todo[inline]{Is it rate or throughput? Clipper uses the latter}, and a user-defined slack parameter.
%In our estimation, we use a slack parameter of $10$.
%We estimate the throughput and latency based on the time it takes for the deployment platform to predict the labels of the evaluation dataset.
%The evaluation dataset contains 2 million data points.
%The deployment platform is queried using the evaluation dataset every minute and requires $15$ seconds to return the predictions in the worst case scenario (when the evaluation dataset is stored on disk). 
%This amounts to a latency of $7 * 10 ^ {(-6)}$ seconds (7.5 micro seconds) and a throughput of $34,000$ requests per second.
%
%Based on above parameters, the scheduler computes the scheduling intervals for every execution of the proactive training.
%
%\begin{figure}[h!]
%\centering
%\includegraphics[width=\columnwidth]{../images/experiment-results/criteo-scheduling-experiment.eps}
%\caption{Analysis of scheduling policy}
%\label{fig:scheduling-policy-time}
%\vspace{2mm}
%\end{figure}
%
%Using the slack parameter, we can guide the scheduler to increase or decrease the scheduling intervals of the proactive training.
%The slack parameter allows for the deployment platform to accommodate surges in the incoming prediction requests and new training data.
%In scenarios where sudden surges are expected (e.g., online stores), we recommend a large slack parameter (recommended value is $10$). \todo[inline]{No exp. validate this}

\subsection{EX3: Optimizations Effects}
In this experiment, we analyze the effects of the optimizations, online statistics computation and materialization of features, on the end-to-end deployment time.
Figure \ref{optimization-effect} shows the effect of optimizations on total training time.
By materializing the features, new instances of the proactive training have access directly to the preprocessed data.
As a result, proactive training directly computes the gradient and updates the deployed model.
For the URL pipeline, our optimizations have reduced the continuous training time by 50\%.
\hl{I will update Figure} \ref{optimization-effect} \hl{and show the time for every component of the pipeline and how the optimizations helped in reducing the time.}
\begin{figure}[h!]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/optimization-time-experiment}}
\caption{Effect of the optimizations on total training time in continuous deployment approach}
\label{optimization-effect}
\end{figure}

\subsection{Discussion} \label{subsec:discussion}
\todo[inline]{Maybe I can also add the result of the scheduling experiment}
\todo[inline]{Explain it is always possible to train the model further and may achieve better result in periodical deployment by given the same amount of time in tuning and designing a pipeline, continuous performs better.}
\textbf{Trade off between quality and training cost}

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/quality-vs-time}}
\caption{Trade off between average quality of the deployed model and total cost of training}
\label{sampling-method-figure}
\end{figure}

\textbf{Side effect of periodical deployment.}
In our experiments, incoming predictions where paused until the end of a periodical training.
However, in real-world scenarios, prediction queries arrive constantly arrive at the system.
Therefore, prediction queries are answered by the model before the periodical training was initiated. 
Moreover, during the periodical training, the model cannot be updated using online learning algorithm as well.
As a result, the performance of the model while a new model is being trained does not improve.
The average training time for the proactive training is very small (200 ms for the URL pipeline \hl{nyc and criteo}).
Therefore, predictions queries are always answered by an up-to-date model and online learning is not paused in the continuous deployment.


