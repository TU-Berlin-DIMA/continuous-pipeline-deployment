\section{Evaluation} \label{evaluation} 
\begin{figure*}[t]
\centering
\resizebox{\textwidth}{!}{\input{../images/experiment-results/tikz/deployment-quality-and-time-experiment}}
 \vspace{-20pt}
\caption{Model Quality and Training cost for different deployment approaches}
 \vspace{-10pt}
\label{deployment-quality-figure}
\end{figure*}
To evaluate the performance of our deployment platform, we perform several experiments.
Our main goal is to show that the continuous deployment approach maintains the quality of the deployed model while reducing the total training time.
Specifically, we answer the following questions:\\
1. How does our continuous deployment approach perform in comparison to online and periodical deployment approaches with regards to model quality and training time? \\
2. What are the effects of the learning rate adaptation method, the regularization parameter, and the sampling strategy on the continuous deployment? \\
3. What are the effects of online statistics computation and dynamic materialization optimizations on the training time?

To that end, we first design two pipelines each processing one real-world dataset.
Then, we deploy the pipelines using different deployment approaches.
\subsection{Setup}\label{subsec:setup}
\textbf{Pipelines.}
We design two pipelines for all the experiments.

\textit{URL pipeline.} The URL pipeline processes the URL dataset for classifying URLs, gathered over a 121 days period, into malicious and legitimate groups \cite{ma2009identifying}.
The pipeline consists of 5 components: input parser, missing value imputer, standard scaler, feature hasher, and an SVM model.
To evaluate the SVM model, we compute the misclassification rate on the unseen data.

\textit{Taxi Pipeline.} The Taxi pipeline processes the New York taxi trip dataset and predicts the trip duration of every taxi ride \cite{newyork-taxi}. 
The pipeline consists of 5 components: input parser, feature extractor, anomaly detector, standard scaler, and a Linear Regression model.
We design the pipeline based on the solutions of the top scorers of the New York City (NYC) Taxi Trip Duration Kaggle competition\footnote{https://www.kaggle.com/c/nyc-taxi-trip-duration/}. 
The input parser computes the actual trip duration by first extracting the pickup and drop off time fields from the input records and calculating the difference (in seconds) between the two values.
The feature extractor computes the haversine distance\footnote{https://en.wikipedia.org/wiki/Haversine\_formula}, the bearing\footnote{https://en.wikipedia.org/wiki/Bearing\_(navigation)}, the hour of the day, and the day of the week from the input records. 
Finally, the anomaly detector filters the trips that are longer than 22 hours, smaller than 10 seconds, or the trips that have a total distance of zero (the car never moved).
To evaluate the model, we use the Root Mean Squared Logarithmic Error (RMSLE) measure.
RMSLE is also the chosen error metric for the NYC Taxi Trip Duration Kaggle competition.

\textbf{Deployment Environment. }
We deploy the URL pipeline on a single laptop running a macOS High Sierra 10.13.4 with 2,2 GHz Intel Core i7, 16 GB of RAM, and 512GB SSD and the Taxi pipeline on a cluster of 21 machines (Intel Xeon 2.4 GHz 16 cores, 28 GB of dedicated RAM per node).
In our current prototype, we are using Apache Spark 2.2 as the execution engine.
The data manager component utilizes the Hadoop Distributed File System (HDFS) 2.7.1 for storing the historical data \cite{shvachko2010hadoop}.
We leverage the SVM, LogisticRegression, and the GradientDescent classes of the machine learning library in Spark (MLlib) to implement the proactive training logic.
We represent both the raw data and the feature chunks as RDDs.
Therefore, we can utilize the caching mechanism of Apache Spark to simply materialize/dematerialize feature chunks.

\textbf{Datasets. }
Table \ref{dataset-description} describes the details of the datasets such as the size of the raw data for the initial training, and the amount of data for the prediction queries and further training after deployment. 
For the URL pipeline, we first train a model on the first day of the data (day 0).
For the Taxi pipeline, we train a model using the data from January 2015.
For both datasets, since the entire data fits in the memory of the computing nodes, we use batch gradient descent (sampling ratio of 1.0) during the initial training.
We then deploy the models (and the pipelines).
We use the remaining data for sending prediction queries and further training of the deployed models.
\begin{table}[h!]
\centering
\begin{tabular}{lrrll}
\hline
\textbf{Dataset}  & \textbf{size} &\textbf{\# instances} & \textbf{Initial} & \textbf{Deployment} \\
\hline
URL        &  2.1 GB 	& 2.4 M  			& Day 0        	  & Day 1-120          \\
Taxi        &  42 GB 	    & 280 M            & Jan15              & Feb15 to Jun16    \\
\hline
\end{tabular}
\caption{Description of Datasets. The Initial and Deployment columns indicate the amount of data used during the initial model training and the deployment phase (prediction queries and further training data)}  
 \vspace{-25pt}
\label{dataset-description}
\end{table}

\textbf{Evaluation metrics. }
For experiments that compare the quality of the deployed model, we utilize the prediction queries to compute the cumulative prequential error rate of the deployed models over time \cite{dawid1984present}.
For experiments that capture the cost of the deployment, we measure the time the platforms spend in updating the model, performing proactive training (retraining for the periodical deployment scenario), and answering prediction queries.

\textbf{Deployment process.}
The URL dataset does not have timestamps. 
Therefore, we divide every day of the data into chunks of 1 minute which results in a total of 12000 chunks, each one with the size of roughly 200KB.
The deployment platform first uses the chunks for prequential evaluation and then updates the deployed model.
The Taxi dataset includes timestamps. 
In our experiments, each chunk of the Taxi dataset contains one hour of the data, which results in a total of 12382 chunks, with an average size of 3MB per chunk. 
The deployment platform processes the chunks in order of the timestamps (from 2015-Feb-01  00:00 to 2016-Jun-30 24:00, an 18 months period).

\subsection{Experiment 1: Deployment Approaches}
In this experiment, we investigate the effect of our continuous deployment approach on model quality and the total training time.
We use 3 different deployment approaches.
\begin{itemize}
\item Online: deploy the pipeline, then utilize online gradient descent with Adam learning rate adaptation method for updating the deployed model.
\item Periodical: deploy the pipeline, then periodically retrain the deployed model.
\item Continuous: deploy the pipeline, then continuously update the deployed model using our platform.
\end{itemize}

The periodical deployment initiates a full retraining every 10 days and every month for URL and Taxi pipelines, respectively.
Since the rate of the incoming training and prediction queries are known, we use static scheduling for the proactive training.
Based on the size and rate of the data, our deployment platform executes the proactive training every 5 minutes and 5 hours for the URL and Taxi pipelines, respectively.
To improve the performance of the periodical deployment, we utilize the warm starting technique, used in the TFX framework \cite{baylor2017tfx}.
In warm starting, each periodical training uses the existing parameters such as the pipeline statistics (e.g., standard scaler), model weights, and learning rate adaptation parameters (e.g., the average of past gradients used in Adadelta, Adam, and Rmsprop) when training new models.

Figure \ref{deployment-quality-figure} (a) and (c) show the cumulative error rate over time for the different deployment approaches.
For both datasets, the continuous and the periodical deployment result in a lower error rate than the online deployment.
Online deployment visits every incoming training data point only once.
As a result, the model updates are more prone to noise.
This results in a higher error rate than the continuous and periodical deployment.
In Figure \ref{deployment-quality-figure} (a), during the first 110 days of the deployment, the continuous deployment has a lower error rate than the periodical deployment.
Only after the final retraining, the periodical deployment slightly outperforms the continuous deployment.
However, from the start to the end of the deployment process, the continuous deployment improves the average error rate by 0.3\% and 1.5\% over the periodical and online deployment, respectively.
In Figure \ref{deployment-quality-figure}  (c), for the Taxi dataset, the continuous deployment always attains a smaller error rate than the periodical deployment.
Overall, the continuous deployment improves the error rate by 0.05\% and  0.1\% over the periodical and online deployment, respectively.

When compared to the online deployment, periodical deployment slightly decreases the error rate after every retraining.
However, between every retraining, the platform updates the model using online learning.
This contributes to the higher error rate than the continuous deployment, where the platform continuously trains the deployed model using samples of the historical data.

In Figure \ref{deployment-quality-figure} (b) and (d), we report the cumulative cost over time for every deployment platform.
We define the deployment cost as the total time spent in data preprocessing, model training, and performing prediction.
For the URL dataset (Figure \ref{deployment-quality-figure} (b)), online deployment has the smallest cost (around 34 minutes) as it only scans each data point once (around 2.4 million scans).  
The continuous deployment approach scans 45 million data points.
However, the total cost at the end of the deployment is only 50\% larger than the online deployment approach (around 54 minutes).  
Because of the online statistics computation and the dynamic materialization optimizations, a large part of the data preprocessing time is avoided.
For the periodical deployment approach, the cumulative deployment cost starts similar to the online deployment approach.
However, after every offline retraining, the deployment cost substantially increases.
At the end of the deployment process, the total cost for the periodical deployment is more than 850 minutes which is 15 times more than the total cost of the continuous deployment approach.
Each data point in the URL dataset has more than 3 million features.
Therefore, the convergence time for each retraining is very high.
The high data-dimensionality and repeated data preprocessing contribute to the large deployment cost of the periodical deployment.

For the Taxi dataset (Figure \ref{deployment-quality-figure} (d)), the cost of online, continuous, and periodical deployments are 262, 308, and 1765 minutes, respectively.
Similar to the URL dataset, continuous deployment only adds a small overhead to the deployment cost when compared with the online deployment.
Contrary to the URL dataset, the feature size of the Taxi dataset is 11.
Therefore, offline retraining converges faster to a solution.
As a result, for the Taxi dataset, the cost of the periodical deployment is 6 times larger than the continuous deployment (instead of 15 times for URL dataset). 
\begin{table*}[t]
\centering
\begin{adjustbox}{max width=\textwidth}
\input{../images/experiment-results/tikz/parameter-selection-table}
\end{adjustbox}
\caption{Hyperparameter tuning during initial training (bold numbers show the best results for each adaptation techniques)}
 \vspace{-25pt}
\label{hyper-param-table}
\end{table*}
\subsection{Experiment 2: System Tuning}
\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/parameter-selection-figure}}
 \vspace{-15pt}
\caption{Result of hyperparameter tuning during the deployment}
 \vspace{-10pt}
\label{hyper-param-figure}
\end{figure}
\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/sampling-mode-figure}}
 \vspace{-15pt}
\caption{Effect of different sampling methods on quality}
 \vspace{-15pt}
\label{sampling-method-figure}
\end{figure}
In this experiment, we investigate the effect of different parameters on the quality of the models after deployment.
As described in Section \ref{proactive-training}, proactive training is an extension of the stochastic gradient descent to the deployment phase.
Therefore, we expect the set of hyperparameters with the best performance during the initial training also performs the best during the deployment phase.

\textbf{Proactive Training Parameters. }
Stochastic gradient descent is heavily dependent on the choice of learning rate and the regularization parameter.
To find the best set of hyperparameters for the initial training, we perform a grid search.
We use advanced learning rate adaptation techniques (Adam, Adadelta, and Rmsprop) for both initial and proactive training.
For each dataset, we divide the initial data (from Table \ref{dataset-description}) into a training and evaluation set.
For each configuration, we first train a model using the training set and then evaluate the model using the evaluation set.
Table \ref{hyper-param-table} shows the result of the hyperparameter tuning for every pipeline.
For the URL dataset, Adam with regularization parameter $1E{\text -}3$ yields the model with the lowest error rate.
The Taxi dataset is less complex than the URL dataset and has a smaller number of feature dimensions.
As a result, the choice of different hyperparameter does not have a large impact on the quality of the model.
The Rmsprop adaptation technique with the regularization parameter of $1E{\text -}4$ results in a slightly better model than the other configurations.

After the initial training, for every configuration, we deploy the model and use 10 \% of the remaining data to evaluate the model after deployment.
Figure \ref{hyper-param-figure} shows the results of the different hyperparameter configurations on the deployed model.
To make the deployment figure more readable, we avoid displaying the result of every possible combination of hyperparameters and only show the result of the best configuration for each learning rate adaptation technique.
For the URL dataset, similar to the initial training, Adam with regularization parameter $1E{\text -}3$ results in the best model.
For the Taxi dataset, we observe a similar behavior to the initial training where different configurations do not have a significant impact on the quality of the deployed model.

This experiment confirms that the effect of the hyperparameters (learning rate and regularization) during the initial and proactive training are the same.
Therefore, we tune the parameters of the proactive training based on the result of the hyperparameter search during the initial training.

\textbf{Sampling Methods.}
The choice of the sampling strategy also affects the proactive training.
Each instance of the proactive training updates the deployed model using the provided sample.
Therefore, the quality of the model after an update is directly related to the quality of the sample.
We evaluate the effect of three different sampling strategies, namely, time-based, window-based, and uniform, on the quality of the deployed model.
The sample size is similar to the sample size during the initial training ($16k$ and $1M$ for URL and Taxi data, respectively).
Figure \ref{sampling-method-figure} shows the effect of different sampling strategies on the quality of the deployed model.
For the URL dataset, time-based sampling improves the average error rate by 0.5\% and 0.9\% over the window-based and uniform sampling, respectively.
As new features are added to the URL dataset over time, the underlying characteristics of the dataset gradually change \cite{ma2009identifying}.
A time-based sampling approach is more likely to select the recent items for the proactive training.
As a result, the deployed model performs better on the incoming prediction queries.
The underlying characteristics of the Taxi dataset are known to remain static over time.
As a result, we observe that different sampling strategies have the same effect on the quality of the deployed model.

Our experiments show that for datasets that gradually change over time, time-based sampling outperforms other sampling strategies.
Moreover, time-based sampling performs similarly to window-based and uniform sampling for datasets with stationary distributions.
\subsection{Experiment 3: Optimizations Effects}
In this experiment, we analyze the effect of the system optimizations, i.e., online statistics computation and the dynamic materialization on the total deployment cost.
We define the materialization rate as the ratio of the number of materialized chunks over the total number of chunks (both URL and Taxi have around 12,000 chunks in total).
For both datasets, the materialization rates of 0.0, 0.2, 0.6, and 1.0 indicates that 0, 2400, 7200, and 12000 chunks are materialized.
For the window-based sampling strategy, we set the window size to 6,000 chunks (half of the total chunks).
\begin{table}[!h]
 \vspace{-9pt}
\begin{tabular}{lllll}
\hline
& \multicolumn{2}{c}{\textbf{URL}}  & \multicolumn{2}{c}{\textbf{Taxi}} \\
 \cmidrule(lr){2-3} \cmidrule(lr){4-5} 
\textbf{Sampling}  & \multicolumn{1}{l}{m=0.2} & \multicolumn{1}{l}{m=0.6} 
 &  \multicolumn{1}{l}{m=0.2} & \multicolumn{1}{l}{m=0.6}  \\
\hline
Uniform      	&  0.52 & 0.91 & 0.51 & 0.90 \\
Window-based &  0.58 	& 1.0 & 0.57 & 1.0 \\
Time-based &   0.68 	&  0.97 & 0.65 & 0.97 \\ \hline      
\end{tabular}
\caption{Empirical computation of $MU$ for different sampling strategies and materialization rates (m).}
 \vspace{-23pt}
\label{table-empirical-mu}
\end{table}
Using the Formula \ref{formula-uniform} and \ref{formula-window-based} of Section \ref{subsec:dynamic-materialization}, when the materialization rate is 0.0, 0.2, 0.6, and 1.0 the average materialization utilization rate ($MU$) is 0.0, 0.5218, 0.9064, and 1.0 for uniform sampling and 0.0, 0.5832, 1.0, and 1.0 for window-based sampling.
To validate our analysis, we compute $MU$ empirically as well.
Table \ref{table-empirical-mu} shows the value of $MU$ for different settings.
For both the uniform and time-based sampling, the empirical and analytical computation yield similar values.
Moreover, the empirical computation shows that the time-based strategy performs better than the uniform sampling strategy.
When the number of materialized feature chunks is 0 or 12000, the design of the deployment platform guarantees that $MU$ is 0.0 and 1.0, respectively. 
Therefore, we do not report those results in the table.
\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/dynamic-materialization-experiment}}
 \vspace{-15pt}
\caption{Effect of the online statistics computation and dynamic materialization on the deployment cost}
 \vspace{-21pt}
\label{fig-optimization-effect}
\end{figure}

To examine the effect of $MU$ on the deployment cost, we plot the total deployment cost using different sampling strategies and materialization rates for the URL and Taxi deployment scenarios in Figure \ref{fig-optimization-effect}.
When the materialization rate is 0.0 or 1.0, the sampling strategies have similar effects on the deployment cost. 
Therefore, the total deployment cost for every sampling strategy is 90 minutes for URL and 600 minutes for Taxi deployment scenario, when the materialization rate is 0.0.
Similarly, the deployment cost is 54 minutes for URL and 308 minutes for Taxi, when the materialization rate is 1.0 (an improvement of 40\% for URL and 49\% for Taxi deployment scenarios).

For the URL deployment scenario, when the materialization rate is 0.2, time-based, window-based, and uniform sampling improve the deployment cost by 30\%, 25\%, and 23\% in comparison with the materialization rate of 0.0.
Similarly, in the Taxi deployment scenario, time-based, window-based, and uniform sampling improve the deployment cost by 22\%, 16\%, and 12\%, respectively. 
Time-based sampling performs better since it has a higher $MU$ value than the other two sampling strategies (Table \ref{table-empirical-mu}).
When the materialization rate is 0.2, the rate of the decrease in the deployment cost for the URL scenario is greater than the Taxi scenario.
We attribute this difference in the decrease in the deployment cost to two reasons.
First, the number of sampled chunks in the Taxi deployment scenario is larger than the URL (720 for Taxi and 100 for URL).
Before updating the model, we utilize the \textit{context.union} operation of Spark, to combine all the non-materialized and materialized chunks.
The union operation incurs a larger overhead when the number of underlying chunks is bigger.
Second, we execute the URL deployment scenario on a single machine with SSD.
Since materializing data that resides on an SSD is faster than an HD, we observe a larger decrease in the deployment cost.

When the materialization rate is 0.6, window-based sampling has the best performance.
Since the size of the window is smaller than the number of the materialized feature chunks, every sampled feature chunk is materialized.
For the URL deployment scenario, window-based, time-based, and uniform sampling improves the performance by 40\%, 36\%, and 33\%, respectively.
For the Taxi deployment scenario, window-based, time-based, and uniform sampling improves the performance by 49\%, 46\%, and 37\%, respectively.
Similar phenomena explain the difference in performance improvement at materialization rate of 0.6 between the Taxi and the URL deployment scenarios.
At materialization rate of 0.6, more than 90\% of the chunks are materialized.
Therefore, the Taxi deployment scenario gains relatively more than the URL deployment scenario from a smaller number of disk I/O operations.

To analyze the effect of the online statistics computation on the deployment cost, we also execute the deployment scenarios without the online statistics computation and the dynamic materialization optimizations.
In this case, the deployment platform first accesses the sampled raw data chunk directly from the disk.
Then, the platform recomputes the required statistics of every component by scanning the data.
Finally, it transforms the raw data chunk into the preprocessed feature chunks by utilizing the deployed pipeline.
Without the optimizations, the choice of the sampling strategy does not affect the total deployment time (similar to the materialization rate of 0.0).
Therefore, when the optimizations are disabled, we only show the results for the time-based sampling (depicted as NoOptimization in Figure \ref{fig-optimization-effect}).
The extra disk access and data processing result in an increase of \%110 for the URL (Figure \ref{fig-optimization-effect}a) and \%170 for the Taxi deployment scenarios (Figure \ref{fig-optimization-effect}b) when compared with a fully optimized execution (with online statistics computation and materialization rate of 1.0).
Similar to the dynamic materialization case, we observe a larger increase in the deployment cost of the Taxi deployment scenario due to the larger overhead of disk I/O.

The result of this experiment shows that even under limited storage we can benefit from the dynamic materialization, especially for the time-based and window-based sampling strategies.
Furthermore, online statistics computation can improve the total deployment cost, especially when the expected amount of incoming data is large.

\subsection{Discussion} \label{subsec:discussion}
\textbf{Trade-off between quality and training cost.}
In many real-world use cases, even a small improvement in the quality of the deployed model can have a significant impact  \cite{ling2017model}.
Therefore, one can employ more complex pipelines and machine learning training algorithms to train better models.
However, during the deployment where prediction queries and training data become available at a high rate, one must consider the effect of the training time.
To ensure the model is always up-to-date, the platform must constantly update the model.
Long retraining time may have a negative impact on the prediction accuracy as the deployed model becomes stale.
Figure \ref{trade-off-figure} shows the trade-off between the average quality and the total cost of the deployment.
By utilizing continuous deployment, we improve the average quality by 0.05\% and 0.3\% for the Taxi and URL datasets over the periodical deployment approach.
We also reduce the cost of the deployment 6 to 15 times when compared with periodical deployment.
\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/quality-vs-time}}
\caption{Trade-off between average quality and deployment cost}
 \vspace{-10pt}
\label{trade-off-figure}
\end{figure}

\textbf{Staleness of the model during the periodical deployment.}
In the experiments of the periodical deployment approach, we pause the inflow of the training data and prediction queries.
However, in real-world scenarios, the training data and the prediction queries constantly arrive at the platform.
Therefore, the periodical deployment platform pauses the online update of the deployed model and answers the prediction queries using the currently deployed model (similar to how Velox operates \cite{crankshaw2014missing}).
As a result, the error rate of the deployed model may increase during the retraining process.
However, in our continuous deployment platform, the average time for the proactive training is small (200 ms for the URL dataset and 700 ms for the Taxi dataset).
Therefore, the continuous deployment platform always performs the online model update and answers the predictions queries using an up-to-date model.