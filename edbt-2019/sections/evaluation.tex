\section{Evaluation} \label{evaluation} 
To evaluate the performance of our deployment platform, we perform several experiments.
Our main goal is to show that the continuous deployment approach maintains the quality of the deployed model while reducing the total training time.
Specifically, we answer the following questions:\\
1. How does our continuous deployment approach perform in comparison to online and periodical deployment approaches with regards to model quality and training time? \\
2. What are the effects of the learning rate adaptation method, the regularization parameter, and the sampling strategy on the continuous deployment? \\
3. What are the effects of online statistics computation and dynamic materialization optimizations on the training time?

To that end, we first design two pipelines each processing one real-world dataset.
Then, we deploy the pipelines using different deployment approaches.

\subsection{Setup}\label{subsec:setup}
\textbf{Pipelines.}
We design two pipelines for all the experiments.

\textit{URL pipeline.} The URL pipeline processes the URL dataset for classifying URLs, gathered over a 121 days period, into malicious and legitimate groups \cite{ma2009identifying}.
The pipeline consists of 5 components: input parser, missing value imputer, standard scaler, feature hasher, and an SVM model.
To evaluate the SVM model, we compute the misclassification rate on the unseen data.

\textit{Taxi Pipeline.} The Taxi pipeline processes the New York taxi trip dataset and predicts the trip duration of every taxi ride \cite{newyork-taxi}. 
The pipeline consists of 5 components: input parser, feature extractor, anomaly detector, standard scaler, and a Linear Regression model.
We design the pipeline based on the solutions of the top scorers of the New York City (NYC) Taxi Trip Duration Kaggle competition\footnote{https://www.kaggle.com/c/nyc-taxi-trip-duration/}. 
The input parser computes the actual trip duration by first extracting the pickup and drop off time fields from the input records and calculating the difference (in seconds) between the two values.
The feature extractor computes the haversine distance\footnote{https://en.wikipedia.org/wiki/Haversine\_formula}, the bearing\footnote{https://en.wikipedia.org/wiki/Bearing\_(navigation)}, the hour of the day, and the day of the week from the input records. 
Finally, the anomaly detector filters the trips that are longer than 22 hours, smaller than 10 seconds, or the trips that have a total distance of zero (the car never moved).
To evaluate the model, we use the Root Mean Squared Logarithmic Error (RMSLE) measure.
RMSLE is also the chosen error metric for the NYC Taxi Trip Duration Kaggle competition.

\textbf{Deployment Environment. }
We deploy the URL pipeline on a single laptop running a macOS High Sierra 10.13.4 with 2,2 GHz Intel Core i7, 16 GB of RAM, and 512GB SSD and the Taxi pipeline on a cluster of 21 machines (Intel Xeon 2.4 GHz 16 cores, 28 GB of dedicated RAM per node).
In our current prototype, we are using Apache Spark 2.2 as the execution engine.
The data manager component utilizes the Hadoop Distributed File System (HDFS) 2.7.1 for storing the historical data \cite{shvachko2010hadoop}.
\hl{We leverage the SVM, LogisticRegression, and the GradientDescent classes of the machine learning library in Spark (MLlib) to implement the proactive training logic.
We represent both the raw data and the feature chunks as RDDs.
Therefore, we can utilize the caching mechanism of Apache Spark to simply materialize/unmaterialize feature chunks.}

\hl{\textbf{Datasets. }}
Table \ref{dataset-description} describes the details of the datasets such as the size of the raw data for the initial training, and the amount of data for the prediction queries and further training after deployment. 
For the URL pipeline, we first train a model on the first day of the data (day 0).
For the Taxi pipeline, we train a model using the data from January 2015.
For both datasets, since the entire data fits in the memory of the computing nodes, we use batch gradient descent (sampling ratio of 1.0) during the initial training.
We then deploy the models (and the pipelines).
We use the remaining data for sending prediction queries and further training of the deployed models.

\hl{\textbf{Evaluation metrics. }}
For experiments that compare the quality of the deployed model, we utilize the prediction queries to compute the cumulative prequential error rate of the deployed models over time \cite{dawid1984present}.
For experiments that capture the cost of the deployment, we measure the time the platforms spend in updating the model, performing proactive training (retraining for the periodical deployment scenario), and answering prediction queries.

\hl{
\textbf{Deployment process.}
The URL dataset does not have timestamps. 
Therefore, we divide every day of the data into chunks of 1 minute which results in a total of 12000 chunks, each one with the size of roughly 200KB.
The deployment platform first uses the chunks for prequential evaluation and then updates the deployed model.
The Taxi dataset includes timestamps. 
In our experiments, each chunk of the Taxi dataset contains one hour of the data, which results in a total of 12382 chunks, with an average size of 3MB per chunk. 
The deployment platform processes the chunks in order of the timestamps (from 2015-Feb-01  00:00 to 2016-Jun-30 24:00, an 18 months period).}

\begin{table}[h!]
\centering
\begin{tabular}{lrrll}
\hline
\textbf{Dataset}  & \textbf{size} &\textbf{\# instances} & \textbf{Initial} & \textbf{Deployment} \\
\hline
URL        &  2.1 GB 	& 2.4 M  			& Day 0        	  & Day 1-120          \\
Taxi        &  42 GB 	    & 280 M            & Jan15              & Feb15 to Jun16    \\
\hline
\end{tabular}
\caption{Description of Datasets. The Initial and Deployment columns indicate the amount of data used during the initial model training and the deployment phase (prediction queries and further training data)}  
\label{dataset-description}
\end{table}

\subsection{Experiment 1: Deployment Approaches}
\begin{figure*}[t]
\centering
\resizebox{\textwidth}{!}{\input{../images/experiment-results/tikz/deployment-quality-and-time-experiment}}
\caption{Model Quality and Training cost for different deployment approaches}
\label{deployment-quality-figure}
\end{figure*}

In this experiment, we investigate the effect of our continuous deployment approach on model quality and the total training time.
We use 3 different deployment approaches.
\begin{itemize}
\item Online: deploy the pipeline, then utilize online gradient descent with Adam learning rate adaptation method for updating the deployed model.
\item Periodical: deploy the pipeline, then periodically retrain the deployed model.
\item Continuous: deploy the pipeline, then continuously update the deployed model using our platform.
\end{itemize}

The periodical deployment initiates a full retraining every 10 days and every month for URL and Taxi pipelines, respectively.
Since the rate of the incoming training and prediction queries are known, we use static scheduling for the proactive training.
Based on the size and rate of the data, our deployment platform executes the proactive training every 5 minutes and 5 hours for the URL and Taxi pipelines, respectively.
To improve the performance of the periodical deployment, we utilize the warm starting technique, used in the TFX framework \cite{baylor2017tfx}.
In warm starting, each periodical training uses the existing parameters such as the pipeline statistics (e.g., standard scaler), model weights, and learning rate adaptation parameters (e.g., the average of past gradients used in Adadelta, Adam, and Rmsprop) when training new models.

Figure \ref{deployment-quality-figure} (a) and (c) show the cumulative error rate over time for the different deployment approaches.
For both datasets, the continuous and the periodical deployment result in a lower error rate than the online deployment.
Online deployment visits every incoming training data point only once.
As a result, the model updates are more prone to noise.
This results in a higher error rate than the continuous and periodical deployment.
In Figure \ref{deployment-quality-figure} (a), during the first 110 days of the deployment, the continuous deployment has a lower error rate than the periodical deployment.
Only after the final retraining, the periodical deployment slightly outperforms the continuous deployment.
However, from the start to the end of the deployment process, the continuous deployment improves the average error rate by 0.3\% and 1.5\% over the periodical and online deployment, respectively.
In Figure \ref{deployment-quality-figure}  (c), for the Taxi dataset, the continuous deployment always attains a smaller error rate than the periodical deployment.
Overall, the continuous deployment improves the error rate by 0.05\% and  0.1\% over the periodical and online deployment, respectively.

When compared to the online deployment, periodical deployment slightly decreases the error rate after every retraining.
However, between every retraining, the platform updates the model using online learning.
This contributes to the higher error rate than the continuous deployment, where the platform continuously trains the deployed model using samples of the historical data.

In Figure \ref{deployment-quality-figure} (b) and (d), we report the cumulative cost over time for every deployment platform.
We define the deployment cost as the total time spent in data preprocessing, model training, and performing prediction.
For the URL dataset (Figure \ref{deployment-quality-figure} (b)), online deployment has the smallest cost (around 34 minutes) as it only scans each data point once (around 2.4 million scans).  
The continuous deployment approach scans 45 million data points.
However, the total cost at the end of the deployment is only 50\% larger than the online deployment approach (around 54 minutes).  
Because of the online statistics computation and the dynamic materialization optimizations, a large part of the data preprocessing time is avoided.
For the periodical deployment approach, the cumulative deployment cost starts similar to the online deployment approach.
However, after every offline retraining, the deployment cost substantially increases.
At the end of the deployment process, the total cost for the periodical deployment is more than 850 minutes which is 15 times more than the total cost of the continuous deployment approach.
Each data point in the URL dataset has more than 3 million features.
Therefore, the convergence time for each retraining is very high.
The high data-dimensionality and repeated data preprocessing contribute to the large deployment cost of the periodical deployment.

For the Taxi dataset (Figure \ref{deployment-quality-figure} (d)), the cost of online, continuous, and periodical deployments are 262, 308, and 1765 minutes, respectively.
Similar to the URL dataset, continuous deployment only adds a small overhead to the deployment cost when compared with the online deployment.
Contrary to the URL dataset, the feature size of the Taxi dataset is 11.
Therefore, offline retraining converges faster to a solution.
As a result, for the Taxi dataset, the cost of the periodical deployment is 6 times larger than the continuous deployment (instead of 15 times for URL dataset). 

\subsection{Experiment 2: System Tuning}
\begin{table*}[t]
\centering
\begin{adjustbox}{max width=\textwidth}
\input{../images/experiment-results/tikz/parameter-selection-table}
\end{adjustbox}
\caption{Hyperparameter tuning during initial training (bold numbers show the best results for each adaptation techniques)}
\label{hyper-param-table}
\end{table*}
In this experiment, we investigate the effect of different parameters on the quality of the models after deployment.
As described in Section \ref{proactive-training}, proactive training is an extension of the stochastic gradient descent to the deployment phase.
Therefore, we expect the set of hyperparameters with best performance during the initial training also performs the best during the deployment phase.

\textbf{Proactive Training Parameters. }
Stochastic gradient descent is heavily dependent on the choice of learning rate and the regularization parameter.
To find the best set of hyperparameters for the initial training, we perform a grid search.
We use advanced learning rate adaptation techniques (Adam, Adadelta, and Rmsprop) for both initial and proactive training.
For each dataset, we divide the initial data (from Table \ref{dataset-description}) into a training and evaluation set.
For each configuration, we first train a model using the training set and then evaluate the model using the evaluation set.
Table \ref{hyper-param-table} shows the result of the hyperparameter tuning for every pipeline.
For the URL dataset, Adam with regularization parameter $1E{\text -}3$ yields the model with the lowest error rate.
The Taxi dataset is less complex than the URL dataset and has a smaller number of feature dimensions.
As a result, the choice of different hyperparameter does not have a large impact on the quality of the model.
The Rmsprop adaptation technique with regularization parameter of $1E{\text -}4$ results in a slightly better model than the other configurations.

After the initial training, for every configuration, we deploy the model and use 10 \% of the remaining data to evaluate the model after deployment.
Figure \ref{hyper-param-figure} shows the results of the different hyperparameter configurations on the deployed model.
To make the deployment figure more readable, we avoid displaying the result of every possible combination of hyperparameters and only show the result of the best configuration for each learning rate adaptation technique.
For the URL dataset, similar to the initial training, Adam with regularization parameter $1E{\text -}3$ results in the best model.
For the Taxi dataset, we observe a similar behavior to the initial training where different configurations do not have a significant impact on the quality of the deployed model.

This experiment confirms that the effect of the hyperparameters (learning rate and regularization) during the initial and proactive training are the same.
Therefore, we tune the parameters of the proactive training based on the result of the hyperparameter search during the initial training.
\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/parameter-selection-figure}}
\caption{Result of hyperparameter tuning during the deployment}
\label{hyper-param-figure}
\end{figure}

\textbf{Sampling Methods.}
The choice of the sampling strategy also affects the proactive training.
Each instance of the proactive training updates the deployed model using the provided sample.
Therefore, the quality of the model after an update is directly related to the quality of the sample.
We evaluate the effect of three different sampling strategies, namely, time-based, window-based, and uniform, on the quality of the deployed model.
The sample size is similar to the sample size during the initial training ($16k$ and $1M$ for URL and Taxi data, respectively).
Figure \ref{sampling-method-figure} shows the effect of different sampling strategies on the quality of the deployed model.
For the URL dataset, time-based sampling improves the average error rate by 0.5\% and 0.9\% over the window-based and uniform sampling, respectively.
As new features are added to the URL dataset over time, the underlying characteristics of the dataset gradually change \cite{ma2009identifying}.
A time-based sampling approach is more likely to select the recent items for the proactive training.
As a result, the deployed model performs better on the incoming prediction queries.
The underlying characteristics of the Taxi dataset is known to remain static over time.
As a result, we observe that different sampling strategies have the same effect on the quality of the deployed model.

Our experiments show that for datasets that gradually change over time, time-based sampling outperforms other sampling strategies.
Moreover, time-based sampling performs similarly to window-based and uniform sampling for datasets with stationary distributions.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/sampling-mode-figure}}
\caption{Effect of different sampling methods on quality}
\label{sampling-method-figure}
\end{figure}

\subsection{Experiment 3: Optimizations Effects}
\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/dynamic-materialization-experiment}}
\caption{Effect of the online statistics computation and dynamic materialization on the deployment cost with different sampling strategies}
\label{fig-optimization-effect}
\end{figure}
\hl{
In this experiment, we analyze the effect of the system optimizations, i.e., online statistics computation and the dynamic materialization on the total deployment cost.

In Section \ref{subsec:dynamic-materialization}, we provided mathematical bounds on the benefits of the dynamic optimization for different sampling strategies under limited resources.
To demonstrate the effect of the dynamic optimization empirically, we execute the deployment scenario for both the URL and the Taxi datasets with different sampling strategies (i.e., time-based, uniform, and window-based) with different materialization rate.
We define the materialization rate as the ratio of the number of materialized chunks over the total number of chunks (the total number of chunks for each of the URL and Taxi datasets are 12,000).
Figure \ref{fig-optimization-effect} shows the end-to-end deployment cost using different sampling strategies under limited resources.
We vary the materialization ratio from 0.0 (no chunks are materialized) to 1.0 (every chunk is materialized).

When the materialization rate is 0.0, the platform has to rematerialize every chunk, regardless of the sampling strategy.
When the materialization rate is 1.0, the platform utilizes the feature chunks directly without any rematerialization.
Therefore, the total deployment cost for the different sampling strategies is the same when materialization rate is 0.0 (90 minutes for URL and 600 minutes for Taxi) or 1.0 (54 minutes for URL and 308 minutes for Taxi).
However, when the materialization rate is between 0 and 1, the decrease in the deployment cost for each sampling strategy is different.

The uniform sampling strategy benefits the least from the increase in the materialization rate.
Since the expected number of the sampled materialized feature chunks is directly proportional to the materialization rate, the decrease in the deployment cost is also proportional to the increase in the materialization rate. 
For the URL dataset, the deployment cost of the uniform sampling strategy improves by 14\%, 28\%, and 40\% for the materialization rate of 0.2, 0.6, and 1.0, respectively.}

\todo[inline]{Taxi Data}
\hl{
Since the time-based and the window-based sampling strategies have a higher probability of sampling from the more recent data chunks, they benefit more from increasing the materialization rate.
Moreover, for the window-based sampling strategy, we set the window size for both the URL and the Taxi use-cases to 6000 chunks (half of the total number of chunks).
Therefore, when the materialization is greater than 0.5, the number of materialized chunks is greater than the window size.
As a result, the window-based sampling strategy only samples from materialized chunks.
Figure \ref{fig-optimization-effect}a shows that for the materialization rates of 0.2, 0.6, and 1.0, the deployment cost improves by 20\%, 34\%, and 40\% for the time-based sampling and by 24\%, 40\%, and 40\% for the window-based sampling.}
\todo[inline]{Taxi Data}
\hl{
To analyze the effect of the online statistics computation on the total deployment time, we also execute the deployment scenarios with both the online statistics computation and the dynamic materialization optimizations disabled.
In this case, the deployment platform must first access the sampled raw data chunk directly from the disk.
Then, the platform must recompute the required statistics of every component by scanning the data once.
Finally, it transforms the raw data chunk into the preprocessed feature chunks by utilizing the deployed pipeline.
When all the optimizations are disabled, the choice of the sampling strategy does not affect the total deployment time (similar to the materialization rate of 0.0).
Therefore, when the optimizations are disabled, we only show the results for the time-based sampling (depicted as NoOptimization in Figure \ref{fig-optimization-effect}).
Therefore, when the optimizations are disabled, we only show the results for the time-based sampling (depicted as NoOptimization in Figure \ref{fig-optimization-effect}).
The extra disk access and data processing result in an increase of \%110 for the URL dataset (Figure \ref{fig-optimization-effect}a) and \%170 for the Taxi dataset (Figure \ref{fig-optimization-effect}b) when compared with a fully optimized execution (with online statistics computation and materialization rate of 1.0).
There are two reasons for the larger increase in the deployment cost for the Taxi dataset, when the online statistics computation is disabled.
First, we store the URL dataset on an SSD drive and the Taxi use-case on an HDFS.
Therefore, disk access for the URL dataset is faster than the Taxi dataset.
Second, the sample size in the Taxi use-case is larger than the URL use-case.
As a result, in every instance of the proactive training, the platform reads more data from disk in the Taxi use-case.

The result of this experiment confirms our analysis from Section \ref{subsec:dynamic-materialization}, showing that even under limited storage we can benefit from the dynamic materialization, especially for the time-based and window-based sampling strategies.
Furthermore, online statistics computation can affect the total deployment cost, especially when the expected amount of incoming data is large.}

\subsection{Discussion} \label{subsec:discussion}
\textbf{Trade-off between quality and training cost.}
In many real-world use cases, even a small improvement in the quality of the deployed model can have a significant impact  \cite{ling2017model}.
Therefore, one can employ more complex pipelines and machine learning training algorithms to train better models.
However, during the deployment where prediction queries and training data become available at a high rate, one must consider the effect of the training time.
To ensure the model is always up-to-date, the platform must constantly update the model.
Long retraining time may have a negative impact on the prediction accuracy as the deployed model becomes stale.
Figure \ref{trade-off-figure} shows the trade-off between the average quality and the total cost of the deployment.
By utilizing continuous deployment, we improve the average quality by 0.05\% and 0.3\% for the Taxi and URL datasets over the periodical deployment approach.
We also reduce the cost of the deployment 6 to 15 times when compared with periodical deployment.

\begin{figure}[!h]
\centering
\resizebox{\columnwidth}{!}{\input{../images/experiment-results/tikz/quality-vs-time}}
\caption{Trade-off between average quality and deployment cost}
\label{trade-off-figure}
\end{figure}

\textbf{Staleness of the model during the periodical deployment.}
In the experiments of the periodical deployment approach, we pause the inflow of the training data and prediction queries.
However, in real-world scenarios, the training data and the prediction queries constantly arrive at the platform.
Therefore, the periodical deployment platform pauses the online update of the deployed model and answers the prediction queries using the currently deployed model (similar to how Velox operates \cite{crankshaw2014missing}).
As a result, the error rate of the deployed model may increase during the retraining process.
However, in our continuous deployment platform, the average time for the proactive training is small (200 ms for the URL dataset and 700 ms for the Taxi dataset).
Therefore, the continuous deployment platform always performs online model update and answers the predictions queries using an up-to-date model.