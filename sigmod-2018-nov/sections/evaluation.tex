\section{Evaluation} \label{evaluation} 
To evaluate our continuous training approach, we perform several experiments.
We first describe the setup of our experiments including the computing cluster, the deployed pipeline, and how we simulated a real production environment by streaming a real-world large dataset through our deployment platform.
First, we discuss the effect of different parameters (learning rate adaptation, sampling strategy, and scheduling rate) on the quality and training time of the model.
Then, we discuss the effects of proactive training on the quality of the model and compare it to the quality of a model that is trained on a daily basis.
Finally, we evaluate the effects of online statistics computation and data materialization optimizations on the training time.

\subsection{Setup}\label{subsec:setup}
We evaluate our deployment method in distributed environment consists of 21 nodes (1 master, 20 slaves).
Each node is running on an Intel Xeon 2.40 GHz 16 core processor and has 28 GB of dedicated memory for running our prototype.

To demonstrate the deployment platform designed the following machine learning pipeline.

\textbf{Criteo Pipeline.} 
The Criteo pipeline consists of 5 operations: input parser, missing value imputer, standard scaler, one hot encoder, and logistic regression model trainer. 
The Terabyte Criteo click log dataset is used for benchmarking algorithms for clickthrough rate (CTR) prediction \cite{criteo-log}.
It contains 24 days of user click logs. 
The dataset  contains 13 numerical and 26 categorical features. 
In all of our experiments we are using the data from the first 6 days (Day 0 to Day 5) of the Criteo dataset.
Day 0 is used for the initial offline training of the pipeline.
Day 1 to Day 5 are used as a streaming dataset.
To evaluate the quality of the pipeline, we use a sample of the Day 6 to compute the logistic loss.

\textbf{Criteo Data Simulation.}
We simulate a production environment by streaming 5 days of criteo dataset.
The data from each day is divided into 1440 smaller batches and stored on disk.
Each batch represent one minute of data.
We use spark streaming to read the data files one by one and stream them through the deployment platform. 

\subsection{Learning Rate Adaptation Method}
In Section \ref{sgd}, we discussed the importance of learning rate tuning for training a model using the Stochastic Gradient Descent optimization method.
Proactive training is an extension of SGD, therefore the process of tuning the learning rate adaptation method is no different from tuning the it for training.
Figure \ref{fig:criteo-learning-rate} shows the log loss for different learning adaptation methods. 
During the training phase, we evaluated the logistic loss on evaluation dataset after 20, 40, 80, 160, 320, and 500 iterations.
We did not include the results of 
Both Adadelta and Momentum performs very poorly on the criteo data.
Criteo dataset is a complex and high dimensional dataset, where features are a mix of numerical and categorical.
Since categorical features are not standardized, Adadelta and Momentum are not able to effectively tune the learning rate for a mix standardized and non-standardized features.
Both RMSPROP and ADAM manage to effectively reduce the logistic loss during the training phase.
Both methods resulted in the model to converge after 500 iterations.
However, ADAM achieves a lower error rate on the evaluation data when compared to RMSPROP.
After 500 iterations of training, we deploy the pipelines.
We streamed Day 1 of the Criteo dataset to the deployment system and monitored the changes in the loss using the same evaluation dataset.
After deployment, both RMSPROP and Adam further decreased the loss from 0.190 and 0.155 to 0.185 and 0.151 respectively.
Adadelta also reduces the loss by around 0.004, however, the deployed pipeline trained using Adadelta learning rate adaptation technique was not fully converged and as a result the final error rate after the first day is still quite high.
For the rest of our experiments, we chose ADAM as the learning rate adaptation technique.
When using ADAM, the model converges using fewer number of iterations as compared to other learning rate adaptation methods.
Moreover, ADAM is the most effective learning rate for continuous training of the pipeline as well.

\begin{figure}[h!]
\includegraphics[width=\columnwidth]{../images/experiment-results/learning-rate-experiment.eps}
\caption{Learning rate adaptation technique for Criteo pipeline}
\label{fig:criteo-learning-rate}
\end{figure}

\subsection{Sampling Methods}
In this section I will discuss different sampling modes (window based) vs entire history and analyze their effect on quality and training time. 

\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{../images/placeholder.jpeg}
\caption{Effect of Sampling mode on quality}
\label{fig:sampling-mode-quality}
\vspace{2mm}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{../images/placeholder.jpeg}
\caption{Effect of Sampling mode on time}
\label{fig:sampling-mode-time}
\vspace{2mm}
\end{figure}



\subsection{Scheduling Policy}
In this section I will investigate the effect of different scheduling rate.
From the simulation I will check the quality for 10 minute and one hour interval and compare the quality to the daily training.
Then I will use the formula in chapter 3 to do the scheduling and compare the quality of the model using that approach.

\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{../images/placeholder.jpeg}
\caption{Effect of Scheduling rate on quality (10 minutes, 1 hour, automatic)}
\label{fig:scheduling-policy-quality}
\vspace{2mm}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{../images/placeholder.jpeg}
\caption{Effect of Scheduling rate on training time (10 minutes, 1 hour, automatic)}
\label{fig:scheduling-policy-time}
\vspace{2mm}
\end{figure}


\subsection{Proactive Training}
I this section the quality of the model trained using proactive and daily are compared.
\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{../images/experiment-results/criteo-log-loss-continuous-vs-daily.eps}
\caption{Total training time of the pipeline for each scenario}
\label{fig:loss-proactive-vs-daily}
\vspace{2mm}
\end{figure}


\subsection{Model Freshness}
We measure model freshness by two metrics: training recency and rate of new features.
Training recency is determined by the scheduling rate.
Performing more frequent training results in models that can adapt to changes in the data more rapidly.
In Criteo pipeline, we use a feature encoder to transform the categorical features into binary indicator variables.
The initial training data (Day 0) only contains a small portion of all the unique categorical features of the Criteo dataset.
The incoming training data may contain features that have not existed in the dataset before.
Figure \ref{fig:criteo-feature-discovery} shows the feature size overtime for the first 5 days after deployment of Criteo pipeline.
The rate of incoming new features is roughly 30,000 per minute and every day around 45 million new features are generated.
Using our continuous training approach, we update the pipeline as soon as new features become available.
During the next scheduled proactive training, the model is updated using these new features.
As a result, the deployed pipeline is able to answer prediction queries that may contain the same set of features more accurately.
Using a daily training approach, any unseen features that arrive at the system are dropped before a prediction is made.

\begin{figure}[H]
\includegraphics[width=\columnwidth]{../images/experiment-results/feature-discovery-experiment.eps}
\caption{Criteo categorical feature size over time}
\label{fig:criteo-feature-discovery}
\end{figure}



\subsection{Total Training Time}
In this section the total training time for continuous and daily are measured.
I will also switch on and off the two optimizations (stat updates and materialization) for both daily and continuous training.
\begin{figure}[h!]
\centering
\includegraphics[width=\columnwidth]{../images/experiment-results/training-time-experiment.eps}
\caption{Total training time for Continuous and Daily (with and without the two optimizations}
\label{fig:deployment-training-time}
\vspace{2mm}
\end{figure}


\subsection{Discussion} \label{subsec:discussion}
discussion of the results
